{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"EthStaker Guides","text":"<p>Helpful documents for the ETHStaker community.</p> <p>Please use the nav bar on the left. We have guides for Holesky testnet setup, monitoring and alerting, maintenance including security and upgrades, migrating to a larger drive, improving your home network, and in future possibly more.</p>"},{"location":"MEV-relay-list/","title":"MEV relay list for Mainnet","text":"<p>Here is a list of MEV relays for the Ethereum Mainnet network. To add one to your mev-boost configuration, simply copy and paste the Relay URL in your <code>-relays</code> flag value. You can add multiple relays comma-separated to the <code>-relays</code> flag, like this: <code>-relays https://relay1,https://relay2</code>. If you are using multiple relays, the current algorithm for mev-boost will select the relay that offers you the most profit. </p> <p>[!NOTE] Clients like Prysm weigh locally built blocks with a percentage boost for comparision against submitted blocks.   This is used to prioritize local block construction over relay/builder block construction.   Boost is an additional percentage to multiple local block value. Use builder block if:   <code>builder_bid_value * 100 &gt; local_block_value * (local-block-value-boost + 100)</code>  The default weight is <code>10</code> </p> <p>Selecting your relays can be an important decision for some stakers. You should do your own diligence when selecting which relay you want to use.</p> Operator/Relay Name Filtering/Censorship/Compliance MEV Strategies/Ethical considerations Relay software Profit sharing model Builders and searchers Status Payload validated on the relay Notes Support Relay URL Aestus No filtering and no censorship Maximize validator payout by including all available transactions and MEV bundles Aestus' fork of mev-boost-relay 100% to validator Public and permissionless. Dashboard Yes Blog post Twitter Email <code>https://0xa15b52576bcbf1072f4a011c0f99f9fb6c66f3e1ff321f11f461d15e31b1cb359caa092c71bbded0bae5b5ea401aab7e@aestus.live</code> Agnostic Gnosis No filtering and no censorship Maximize validator payout by including all available transactions and MEV bundles Gnosis's fork of mev-boost-relay 100% to validator Public and permissionless. Dashboard Unknown Blog post Discord <code>https://0xa7ab7a996c8584251c8f925da3170bdfd6ebc75d50f5ddc4050a6fdc77f2a3b5fce2cc750d0865e05d7228af97d69561@agnostic-relay.net</code> bloXroute Max Profit Filters out OFAC sanctioned addresses as of Dec 2023 (Called Max profit in the documentation) Maximize validator payout without including transactions and bundles sent from/to wallet addresses that are sanctioned by OFAC bloXroute's fork of mev-boost-relay Unknown Internal and external builders. External searchers. Dashboard Yes Documentation for bloXroute relays offering Discord Email <code>https://0x8b5d2e73e2a3a55c6c87b8b6eb92e0149a125c852751db1422fa951e42a09b82c142c3ea98d0d9930b056a3bc9896b8f@bloxroute.max-profit.blxrbdn.com</code> bloXroute Regulated Filters out OFAC sanctioned addresses (Called Regulated in the documentation) Maximize validator payout without including transactions and bundles sent from/to wallet addresses that are sanctioned by OFAC bloXroute's fork of mev-boost-relay Unknown Internal and external builders. External searchers. Dashboard Yes Documentation for bloXroute relays offering Discord Email <code>https://0xb0b07cd0abef743db4260b0ed50619cf6ad4d82064cb4fbec9d3ec530f7c5e6793d9f286c4e082c0244ffb9f2658fe88@bloxroute.regulated.blxrbdn.com</code> Titan Relay No filtering and no censorship Maximize validator payout by including all available transactions and MEV bundles Helix 100% to validator Internal and external builders. Permissionless Dashboard Yes Titan Relay documentation Discord <code>https://0x8c4ed5e24fe5c6ae21018437bde147693f68cda427cd1122cf20819c30eda7ed74f72dece09bb313f2a1855595ab677d@global.titanrelay.xyz</code> Titan Relay Regional Filters out OFAC sanctioned addresses Maximize validator payout without including transactions and bundles sent from/to wallet addresses that are sanctioned by OFAC Helix 100% to validator Internal and external builders. Permissionless Dashboard Yes Titan Relay documentation Discord <code>https://0x8c4ed5e24fe5c6ae21018437bde147693f68cda427cd1122cf20819c30eda7ed74f72dece09bb313f2a1855595ab677d@regional.titanrelay.xyz</code> Flashbots Filters out OFAC sanctioned addresses (Twitter Screenshot) Maximize validator payout without including transactions and bundles sent from/to wallet addresses that are sanctioned by OFAC mev-boost-relay Specific to builder of bid with highest validator value. 100% to validator from Flashbots builders. Internal and external builders. Permissionless. Dashboard Yes Flashbots documentation Discord <code>https://0xac6e77dfe25ecd6110b8e780608cce0dab71fdd5ebea22a16c0205200f2f8e2e3ad3b71d3499c54ad14d6c21b41a37ae@boost-relay.flashbots.net</code> Manifold Finance SecureRPC No filtering and no censorship Maximize validator payout by including all available private transactions and MEV bundles mev-freelay Varied Internal and external builders. Permissionless. Dashboard Yes Manifold documentation. This relay had a major issue on October 15th 2022 (1, 2). Forum Email <code>https://0x98650451ba02064f7b000f5768cf0cf4d4e492317d82871bdc87ef841a0743f69f0f1eea11168503240ac35d101c9135@mainnet-relay.securerpc.com</code> Ultra Sound Optimistic Priority Relay No filtering and no censorship Maximize validator payout by including all available transactions and MEV bundles mev-boost-relay 100% to validator Public and permissionless. Dashboard Yes Twitter Email <code>https://0xa1559ace749633b997cb3fdacffb890aeebdb0f5a3b6aaa7eeeaf1a38af0a8fe88b9e4b1f61f236d2e64d95733327a62@relay.ultrasound.money</code> Wenmerge No filtering and no censorship Maximize validator payout by including all available transactions and MEV bundles mev-boost-relay 100% to validator from wenmerge builders. Specific to builder of bid with highest validator value. Public and permissionless. Dashboard Yes A relay from Wenmerge to support eth community. Website Twitter Email <code>https://0x8c7d33605ecef85403f8b7289c8058f440cbb6bf72b055dfe2f3e2c6695b6a1ea5a9cd0eb3a7982927a463feb4c3dae2@relay.wenmerge.com</code> Proof Relay Only filtering is availability of ZK Proof for block and validator payment Open auction - maximum value Proof Relay 100% to validator Public and permissionless. Status No - header only Docs Discord <code>https://0xa44f64faca0209764461b2abfe3533f9f6ed1d51844974e22d79d4cfd06eff858bb434d063e512ce55a1841e66977bfd@proof-relay.ponrelay.com</code>"},{"location":"MEV-relay-list/#external-relay-monitoring","title":"External relay monitoring","text":"<ul> <li>MEV Panda by OreoMev</li> <li>calldata.pics by Toni Wahrst\u00e4tter</li> <li>censorship.pics by Toni Wahrst\u00e4tter</li> <li>mevboost.pics by Toni Wahrst\u00e4tter</li> <li>timing.pics by Toni Wahrst\u00e4tter</li> <li>tornado.pics by Toni Wahrst\u00e4tter</li> <li>MEV Watch by Labrys</li> <li>Relays from beaconcha.in</li> <li>Relay Scan from Chris Hager</li> <li>Transparency dashboard by Flashbots</li> <li>Relay Monitor by Metrika</li> <li>Rated Network by Rated Network</li> <li>Inclusion Watch by donnoh.eth and emiliano.eth</li> <li>Neutrality Watch specifically analyses Lido operators. Github.</li> </ul>"},{"location":"MEV-relay-list/#mev-relay-list-for-holesky-testnet","title":"MEV relay list for Holesky testnet","text":"<p>Here is a list of MEV relays for the Ethereum Holesky test network. To add one to your mev-boost configuration, simply copy and paste the Relay URL in your <code>-relays</code> flag value. You can add multiple relays comma-separated to the <code>-relays</code> flag, like this: <code>-relays https://relay1,https://relay2</code>. If you are using multiple relays, the current algorithm for mev-boost will select the relay that offers you the most profit.</p> <p>Selecting your relays can be an important decision for some stakers. You should do your own diligence when selecting which relay you want to use.</p> Operator Notes Relay URL Flashbots <code>https://0xafa4c6985aa049fb79dd37010438cfebeb0f2bd42b115b89dd678dab0670c1de38da0c4e9138c9290a398ecd9a0b3110@boost-relay-holesky.flashbots.net</code> Aestus <code>https://0xab78bf8c781c58078c3beb5710c57940874dd96aef2835e7742c866b4c7c0406754376c2c8285a36c630346aa5c5f833@holesky.aestus.live</code> Titan <code>https://0xaa58208899c6105603b74396734a6263cc7d947f444f396a90f7b7d3e65d102aec7e5e5291b27e08d02c50a050825c2f@holesky.titanrelay.xyz</code> Manifold SecureRPC <code>https://0x94392909bb5b7875ed990c17757ea1602e05e076161e9fc7235a33587ce7ebbe1cc52f0ae3ea28139a7c4b8608dd44d3@holesky-relay.securerpc.com/</code>"},{"location":"MEV-relay-list/#mev-relay-list-for-hoodi-testnet","title":"MEV relay list for Hoodi testnet","text":"<p>Here is a list of MEV relays for the Ethereum Hoodi test network. To add one to your mev-boost configuration, simply copy and paste the Relay URL in your <code>-relays</code> flag value. You can add multiple relays comma-separated to the <code>-relays</code> flag, like this: <code>-relays https://relay1,https://relay2</code>. If you are using multiple relays, the current algorithm for mev-boost will select the relay that offers you the most profit.</p> <p>Selecting your relays can be an important decision for some stakers. You should do your own diligence when selecting which relay you want to use.</p> Operator Notes Relay URL Titan <code>https://0xaa58208899c6105603b74396734a6263cc7d947f444f396a90f7b7d3e65d102aec7e5e5291b27e08d02c50a050825c2f@hoodi.titanrelay.xyz</code>"},{"location":"MEV-relay-list/#mev-relay-list-for-sepolia-testnet","title":"MEV relay list for Sepolia testnet","text":"<p>Here is a list of MEV relays for the Ethereum Sepolia test network. To add one to your mev-boost configuration, simply copy and paste the Relay URL in your <code>-relays</code> flag value. You can add multiple relays comma-separated to the <code>-relays</code> flag, like this: <code>-relays https://relay1,https://relay2</code>. If you are using multiple relays, the current algorithm for mev-boost will select the relay that offers you the most profit.</p> <p>Selecting your relays can be an important decision for some stakers. You should do your own diligence when selecting which relay you want to use.</p> Operator Notes Relay URL Flashbots <code>https://0xafa4c6985aa049fb79dd37010438cfebeb0f2bd42b115b89dd678dab0670c1de38da0c4e9138c9290a398ecd9a0b3110@boost-relay-sepolia.flashbots.net</code>"},{"location":"MEV-relay-list/#configuring-mev-boost-software","title":"Configuring MEV boost software","text":"<p>If you need help installing and configuring mev-boost on your machine, check out our Guide on how to prepare a staking machine for the Merge</p>"},{"location":"alerting/","title":"Guide on how to do alerting for an Ethereum validator","text":"<p>Knowing when there is a potential problem with your machine or your processes is very good practice in IT. It can help you prevent major issues with your system. Alerting is the art of getting notified for when those issues are rising up. A common issue for which alerting is very useful and that many validators are likely to face is low remaining disk space because of the clients' databases growth and the need to do some pruning to control that growth.</p> <p>This guide is meant for people with no or little experience in alerting. This guide will show you step by step how to do alerting on your machine by giving you the instructions to install and configure all the tools needed. It will assume you are using a modern linux distribution with systemd and APT (like Ubuntu 20.04) on a modern x86 CPU (Intel, AMD). It will also use PagerDuty as a easy way to integrate with different messaging services (Email, SMS, Slack, etc) and to manage your incidents. If you do not currently use Prometheus with Node Exporter to monitor your system, you can use my monitoring guide to install and configure those tools before using this guide.</p> <p>A video tutorial of this guide can be seen on https://youtu.be/h2nrVZNofxc .</p>"},{"location":"alerting/#why-would-you-want-to-do-alerting","title":"Why would you want to do alerting?","text":"<p>Here are some good reasons why you might want to do alerting on your machine:</p> <ol> <li>Automatic issue detection: You want to automatically check for issues with your system.</li> <li>Issue prevention: You want to be able to find out about potential issues ahead of time.</li> <li>Issue notification: You want to be automatically notified when there is an issue with your system.</li> </ol>"},{"location":"alerting/#overview","title":"Overview","text":"<p>We will install 1 additional tool to your setup that should already be using Prometheus and Node Exporter: Alertmanager. We will also be using PagerDuty as our incident manager and as our tool to get notified. We will configure Prometheus to check for rules that might indicate a problem with your machine. When a rule is triggered or resolved, Prometheus will send that information to Alertmanage which will route it to PagerDuty.</p> <p>Prometheus is an open-source systems monitoring project. It collects and stores different metrics in a specialized database. It can be configured to constantly verify that certain metrics are within our expected margins. In our setup, it will periodically check for rules and forward any unexpected result to Alertmanager.</p> <p>Alertmanager is an open-source project that handles alerts sent by client applications such as Prometheus. In our setup, it will receive alerts that are triggered and resolved from Prometheus and it will route them to PagerDuty.</p> <p>PagerDuty is a large online incident response platform. It can be used for a wide variety of incident management use cases. In our setup, it will receive alerts from Alertmanager and it will notify us when we have an incident with our machine. Optionally, it can be used to manage your incidents in greater detail.</p> <p></p>"},{"location":"alerting/#setup-an-account-on-pagerduty","title":"Setup an account on PagerDuty","text":"<p>Create an account on https://www.pagerduty.com/ . You can start with the free trial and migrate to the free plan when you are done with the trial.</p> <p>When you eventually reach your Incidents dashboard, make sure to click on the initial test incident and manually resolve it. Click your first service and to go the Integrations tab. Click the + Add another integration and choose Events API V2. Open up the Events API V2 section and copy the value in the Integration Key field. We will need that API key in one configuration file below. It should look like a bunch of random letters and numbers.</p> <p>There are many different ways of configuring PagerDuty to receive a notification when an incident occurs. Play around and check what is available. By default, you should receive an email. It might be interesting to add an SMS notification in there.</p>"},{"location":"alerting/#executing-the-commands","title":"Executing the commands","text":"<p>Almost all of these commands will be performed in a terminal. Start your Terminal application. Any line that starts with the dollar sign (<code>$</code>) is a command that need to be executed in your terminal. Do not input the dollar sign (<code>$</code>) in your terminal, only the text that comes after that.</p> <p>Executing a command with <code>sudo</code> will occasionally ask you for your password. Make sure to enter your account password correctly. You can execute the command again if you fail to enter the correct password after a few attempts.</p>"},{"location":"alerting/#installing-alertmanager","title":"Installing Alertmanager","text":"<p>These steps will install Alertmanager as a systemd service.</p> <p>Create the alertmanager user.</p> <pre><code>$ sudo useradd --no-create-home --shell /bin/false alertmanager\n</code></pre> <p>Create the configuration and data directories with proper ownership.</p> <pre><code>$ sudo mkdir /etc/alertmanager\n$ sudo mkdir /var/lib/alertmanager\n$ sudo chown -R alertmanager:alertmanager /etc/alertmanager\n$ sudo chown -R alertmanager:alertmanager /var/lib/alertmanager\n</code></pre> <p>Download the latest stable version of Alertmanager from https://prometheus.io/download/#alertmanager (avoid any pre-release version). As of this date, the latest version is 0.27.0 . Adjust the following instructions accordingly if there is a newer stable release version with a different archive name. The file name should end with linux-amd64.tar.gz (for linux and AMD64 instructions set).</p> <pre><code>$ wget https://github.com/prometheus/alertmanager/releases/download/v0.27.0/alertmanager-0.27.0.linux-amd64.tar.gz\n</code></pre> <p>Verify that the SHA256 Checksum as shown on https://prometheus.io/download/#alertmanager is the same as the file we just downloaded.</p> <pre><code>$ sha256sum alertmanager-0.27.0.linux-amd64.tar.gz\n</code></pre> <p>Extract the archive.</p> <pre><code>$ tar xvf alertmanager-0.27.0.linux-amd64.tar.gz\n</code></pre> <p>Copy the binaries to the following locations and set ownership.</p> <pre><code>$ sudo cp alertmanager-0.27.0.linux-amd64/alertmanager /usr/local/bin/\n$ sudo cp alertmanager-0.27.0.linux-amd64/amtool /usr/local/bin/\n$ sudo chown alertmanager:alertmanager /usr/local/bin/alertmanager\n$ sudo chown alertmanager:alertmanager /usr/local/bin/amtool\n</code></pre> <p>Remove the download leftovers.</p> <pre><code>$ rm -rf alertmanager-0.27.0.linux-amd64\n$ rm alertmanager-0.27.0.linux-amd64.tar.gz\n</code></pre> <p>Setup the Alertmanager configuration file. Open the YAML config file for editing.</p> <pre><code>$ sudo nano /etc/alertmanager/alertmanager.yml\n</code></pre> <p>Paste the following into the file taking care to replace the placeholder PagerDuty API key by your real API key created in the Setup an account on PagerDuty section above (the <code>&lt;</code> and <code>&gt;</code> characters at the start and at the end of the placeholder should be removed). Exit and save the file.</p> <pre><code>route:\n  group_by: ['alertname']\n  group_wait: 30s\n  group_interval: 5m\n  repeat_interval: 1h\n  receiver: 'pagerduty'\n\nreceivers:\n- name: 'pagerduty'\n  pagerduty_configs:\n  - routing_key: &lt;PagerDuty API key&gt;\n\ninhibit_rules:\n  - source_match:\n      severity: 'critical'\n    target_match:\n      severity: 'warning'\n    equal: ['alertname', 'dev', 'instance']\n</code></pre> <p>Set ownership for the config file.</p> <pre><code>$ sudo chown alertmanager:alertmanager /etc/alertmanager/alertmanager.yml\n</code></pre> <p>Setup the Alertmanager systemd service. Open the service definition file.</p> <pre><code>$ sudo nano /etc/systemd/system/alertmanager.service\n</code></pre> <p>Paste the following into the file. Exit and save the file.</p> <pre><code>[Unit]\nDescription=Alertmanager\nWants=network-online.target\nAfter=network-online.target\n\n[Service]\nType=simple\nUser=alertmanager\nGroup=alertmanager\nRestart=always\nRestartSec=5\nExecStart=/usr/local/bin/alertmanager \\\n    --config.file /etc/alertmanager/alertmanager.yml \\\n    --storage.path /var/lib/alertmanager/ \\\n    --web.listen-address=\"localhost:9093\" \\\n    --cluster.listen-address=\"\"\nExecReload=/bin/kill -HUP $MAINPID\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <p>Reload systemd to reflect the changes.</p> <pre><code>$ sudo systemctl daemon-reload\n</code></pre> <p>Start the service and check the status to make sure it's running correctly.</p> <pre><code>$ sudo systemctl start alertmanager.service\n$ sudo systemctl status alertmanager.service\n</code></pre> <p>Output should look something like this.</p> <pre><code>\u25cf alertmanager.service - Alertmanager\n     Loaded: loaded (/etc/systemd/system/alertmanager.service; enabled; vendor &gt;\n     Active: active (running) since Mon 2021-07-12 22:12:06 UTC; 1s ago\n   Main PID: 81779 (alertmanager)\n      Tasks: 9 (limit: 18405)\n     Memory: 22.3M\n     CGroup: /system.slice/alertmanager.service\n             \u2514\u250081779 /usr/local/bin/alertmanager --config.file /etc/alertmanage&gt;\n\nJul 12 22:12:06 testsystem systemd[1]: Started Alertmanager.\nJul 12 22:12:06 testsystem alertmanager[81779]: level=info ts=2021-07-12T22:1&gt;\nJul 12 22:12:06 testsystem alertmanager[81779]: level=info ts=2021-07-12T22:1&gt;\nJul 12 22:12:06 testsystem alertmanager[81779]: level=info ts=2021-07-12T22:1&gt;\nJul 12 22:12:06 testsystem alertmanager[81779]: level=info ts=2021-07-12T22:1&gt;\nJul 12 22:12:06 testsystem alertmanager[81779]: level=info ts=2021-07-12T22:1&gt;\nJul 12 22:12:06 testsystem alertmanager[81779]: level=info ts=2021-07-12T22:1&gt;\nJul 12 22:12:06 testsystem alertmanager[81779]: level=info ts=2021-07-12T22:1&gt;\nJul 12 22:12:06 testsystem alertmanager[81779]: level=info ts=2021-07-12T22:1&gt;\n</code></pre> <p>If you did everything right, it should say active (running) in green. If not then go back and repeat the steps to fix the problem. Press Q to quit.</p> <p>Enable the Alertmanager service to start on boot.</p> <pre><code>$ sudo systemctl enable alertmanager.service\n</code></pre>"},{"location":"alerting/#configuring-prometheus-to-use-alertmanager","title":"Configuring Prometheus to use Alertmanager","text":"<p>Edit your prometheus configuration file. It's likely in <code>/etc/prometheus/prometheus.yml</code>. If not, adjust accordingly.</p> <pre><code>$ sudo nano /etc/prometheus/prometheus.yml\n</code></pre> <p>Make sure you have the following sections in that configuration file. You might already have part of it in comments. If so, just remove the related comments and paste this in there. This section is often located before the <code>scrape_configs</code> section.</p> <pre><code>alerting:\n  alertmanagers:\n  - static_configs:\n    - targets:\n      - localhost:9093\n\nrule_files:\n  - \"alert_rules.yml\"\n</code></pre>"},{"location":"alerting/#adding-alerting-rules","title":"Adding alerting rules","text":"<p>Setup the rules for alerting. Open the rules file.</p> <pre><code>$ sudo nano /etc/prometheus/alert_rules.yml\n</code></pre> <p>Paste the following base rules into the file. Exit and save the file.</p> <pre><code>groups:\n- name: alert_rules\n  rules:\n  - alert: Available_disk_space_too_low\n    expr: node_filesystem_avail_bytes{mountpoint=\"/\"} &lt;= 85899345920\n    for: 1m\n    labels:\n      severity: critical\n    annotations:\n      summary: Available disk space below 80 GiB\n  - alert: Available_memory_too_low\n    expr: node_memory_MemAvailable_bytes &lt;= 1073741824\n    for: 1m\n    labels:\n      severity: critical\n    annotations:\n      summary: Available memory below 1 GiB\n  - alert: CPU_usage_too_high\n    expr: 100 - (avg by (instance) (irate(node_cpu_seconds_total{mode=\"idle\"}[5m])) * 100) &gt;= 90\n    for: 5m\n    labels:\n      severity: critical\n    annotations:\n      summary: CPU usage above 90%\n</code></pre> <p>This base rules file has 3 rules which you can adjust by modifying the <code>expr</code> field's value.</p> <ol> <li>The first rule will alert you when you have less than around 80 GiB (85899345920 bytes) of available disk space on your <code>/</code> mount continuously for 1 minute. If your filesystem and your partitions are configured in a different way where you want to check for a different mount, you will have to change that <code>expr</code> field. If you have direct access to your prometheus web interface (often at <code>http://&lt;machine ip&gt;:9090</code>), you can execute the <code>node_filesystem_avail_bytes</code> query to view all possible mounts and their current free space. You can also view your current mounts and their free space by running the <code>$ df -h</code> command.</li> <li>The second rule will alert you when you have less than around 1 GiB (1073741824 bytes) of free RAM to be used by your processes continuously for 1 minute. If your machine is consistently using almost all of your available RAM, you might want to lower that 1 GiB (1073741824 bytes) threshold value in that <code>expr</code> field.</li> <li>The third rule will alert you when your CPU cores are used for more than 90% of their processing power continuously for 5 minutes.</li> </ol> <p>Set ownership for the config file. If your prometheus service is running under an account that is not <code>prometheus</code>, adjust accordingly.</p> <pre><code>$ sudo chown prometheus:prometheus /etc/prometheus/alert_rules.yml\n</code></pre> <p>Restart your prometheus service and check the status to make sure it's running correctly. If your prometheus service is not configured to run using systemd with the <code>prometheus.service</code> name, adjust accordingly.</p> <pre><code>$ sudo systemctl restart prometheus.service\n$ sudo systemctl status prometheus.service\n</code></pre> <p>Output should look something like this.</p> <pre><code>\u25cf prometheus.service - Prometheus\n     Loaded: loaded (/etc/systemd/system/prometheus.service; enabled; vendor pr&gt;\n     Active: active (running) since Tue 2021-07-13 01:36:01 UTC; 6s ago\n   Main PID: 83685 (prometheus)\n      Tasks: 12 (limit: 18405)\n     Memory: 125.4M\n     CGroup: /system.slice/prometheus.service\n             \u2514\u250083685 /usr/local/bin/prometheus --config.file /etc/prometheus/pr&gt;\n\nJul 13 01:36:01 testsystem prometheus[83685]: level=info ts=2021-07-13T01:36:&gt;\nJul 13 01:36:02 testsystem prometheus[83685]: level=info ts=2021-07-13T01:36:&gt;\nJul 13 01:36:02 testsystem prometheus[83685]: level=info ts=2021-07-13T01:36:&gt;\nJul 13 01:36:02 testsystem prometheus[83685]: level=info ts=2021-07-13T01:36:&gt;\nJul 13 01:36:02 testsystem prometheus[83685]: level=info ts=2021-07-13T01:36:&gt;\nJul 13 01:36:02 testsystem prometheus[83685]: level=info ts=2021-07-13T01:36:&gt;\nJul 13 01:36:02 testsystem prometheus[83685]: level=info ts=2021-07-13T01:36:&gt;\nJul 13 01:36:02 testsystem prometheus[83685]: level=info ts=2021-07-13T01:36:&gt;\nJul 13 01:36:02 testsystem prometheus[83685]: level=info ts=2021-07-13T01:36:&gt;\nJul 13 01:36:02 testsystem prometheus[83685]: level=info ts=2021-07-13T01:36:&gt;\n</code></pre> <p>If you did everything right, it should say active (running) in green. If not then go back and repeat the steps to fix the problem. Press Q to quit.</p>"},{"location":"alerting/#testing-your-rules","title":"Testing your rules","text":"<p>Time to test some of these rules.</p>"},{"location":"alerting/#available-disk-space-test","title":"Available disk space test","text":"<p>Here is an example to test your Available disk space rule. First, check how much space you have left on your disk.</p> <pre><code>$ df -h\n</code></pre> <p>You should see something like this.</p> <pre><code>Filesystem                         Size  Used Avail Use% Mounted on\nudev                               7.5G     0  7.5G   0% /dev\ntmpfs                              1.6G  1.7M  1.6G   1% /run\n/dev/mapper/ubuntu--vg-ubuntu--lv  915G  438G  431G  51% /\ntmpfs                              7.6G     0  7.6G   0% /dev/shm\ntmpfs                              5.0M     0  5.0M   0% /run/lock\ntmpfs                              7.6G     0  7.6G   0% /sys/fs/cgroup\n/dev/nvme0n1p2                     976M  108M  801M  12% /boot\n/dev/nvme0n1p1                     511M  7.9M  504M   2% /boot/efi\n/dev/loop1                          56M   56M     0 100% /snap/core18/1944\n/dev/loop0                         100M  100M     0 100% /snap/core/11316\n/dev/loop2                         9.2M  9.2M     0 100% /snap/canonical-livepatch/99\n/dev/loop3                          32M   32M     0 100% /snap/snapd/10707\n/dev/loop4                          70M   70M     0 100% /snap/lxd/19188\n/dev/loop5                          33M   33M     0 100% /snap/snapd/12398\n/dev/loop6                          68M   68M     0 100% /snap/lxd/20326\n/dev/loop7                          56M   56M     0 100% /snap/core18/2074\ntmpfs                              1.6G     0  1.6G   0% /run/user/1000\n</code></pre> <p>Here, we can see that the main mount is <code>/</code>. It has around 431GB of available disk space. If you want to test the 80GB limit we set in the alerting rules, we can create a dummy file that is 400GB in size with this command.</p> <pre><code>$ fallocate -l 400G largespacer.img\n</code></pre> <p>That will leave us only around 31GB of available disk space. Waiting around 2 minutes should trigger an incident on PagerDuty and you should receive an email with the alert details.</p> <p>Once your Available disk space test is done, you can remove the dummy file with this command.</p> <pre><code>$ rm largespacer.img\n</code></pre>"},{"location":"alerting/#available-memory-test","title":"Available memory test","text":"<p>Here is an example to test your Available memory rule. Make sure your incident is resolved on PagerDuty first if you just tested your Available disk space rule.</p> <p>Check how much memory you have available.</p> <pre><code>$ free -kh\n</code></pre> <p>You should see something like this.</p> <pre><code>              total        used        free      shared  buff/cache   available\nMem:           15Gi       5.5Gi       334Mi       0.0Ki       9.3Gi       9.3Gi\nSwap:         4.0Gi       114Mi       3.9Gi\n</code></pre> <p>You should check the available column in this output. In this example, we have about 9.3GB of available RAM. We can use the following command to start a dummy process that will use around 8.8GB of memory to reach our less than around 1GB of free RAM threshold.</p> <pre><code>$ &lt;/dev/zero head -c 8800m | tail\n</code></pre> <p>Trying to consume RAM with a dummy process like that can be tricky because of how the operating system manages free memory. If you fail to trigger your rule, try to slowly increment the amount of memory consumed with a new dummy process. This should leave us around 500MB of free RAM. Waiting around 2 minutes should trigger an incident on PagerDuty and you should receive an email with the alert details.</p> <p>Once your Available memory test is done, you can terminate the dummy process that is needlessly consuming your memory by typing <code>CTRL</code>+<code>C</code> in your terminal.</p>"},{"location":"alerting/#resolving-the-incidents","title":"Resolving the incidents","text":"<p>Once the alert is resolved, Alertmanager will call PagerDuty and it should automatically resolve the incident within a few minutes. You can manually acknowledge and resolve any incident on your PagerDuty Incidents dashboard.</p> <p>That's it. That should give you some good alerting foundation. There is a lot more you can do with such setup but we will leave that as an exercise to the reader.</p>"},{"location":"alerting/#security-risks","title":"Security risks","text":"<p>Adding and using Alertmanager, Prometheus and PagerDuty with this configuration comes with a few additional security risks for your system.</p> <p>The first risk comes from the tools themselves. There might be some security issues with them that I am not aware of which might compromise your machine to some malicious actors. A great way to prevent such risk is to keep your system updated.</p> <p>Keeping Prometheus and Alertmanager updated will require some efforts. You will need to monitor new stable releases and whether they include severe or critical bug fixes. To update to a new version, you will need to download the latest stable release, extract the archive, copy the binaries to their expected location and restart these services. The process and the instructions to download the new version, extract the archive and copy the binaries is exactly the same one mentioned at the beginning of the Installing Alertmanager section. You can find the same related section for Prometheus on my monitoring guide. Easy to use script that do all those steps for you are included below.</p> <p>To restart the Alertmanager service after you updated its binary, use this command.</p> <pre><code>$ sudo systemctl restart alertmanager.service\n</code></pre> <p>To restart the Prometheus service after you updated its binary, use this command.</p> <pre><code>$ sudo systemctl restart prometheus.service\n</code></pre> <p>You can find all the Prometheus releases and their changelog on https://github.com/prometheus/prometheus/releases. You can find all the Alertmanager releases and their changelog on https://github.com/prometheus/alertmanager/releases.</p> <p>You might never need to update these tools as there might not be any severe or critical issue with them or there might not be an issue that can be easily exploited by a malicious actor with the version you have. However, it's a good practice to monitor releases for the tools you are using and update them regularly.</p> <p>There might be some risks associated with using PagerDuty. You should contact them and reach their support if you are concerned about the risks associated with that platform.</p> <p>The second risk comes from the additional attack surface that these tools are creating. One of this attack surface is the HTTP server it is adding and the port on which it is listening. This guide configured Alertmanager to only listen on your localhost interface meaning that it cannot be accessed from any other machine on a network. You would have to have malicious processes or actors connecting to Alertmanager from your machine to trigger a false alert for instance. A good way to prevent this risk is to limit the running processes, run only trusted processes, limit who can connect to the machine and only allow trusted people connecting to your machine.</p> <p>There might be other kind of risks associated with those tools and this configuration, but I think these two are the main ones.</p>"},{"location":"alerting/#alertmanager-update-scripts","title":"Alertmanager update scripts","text":"<p>I created a simple script to ease the pain of updating Alertmanager. You can find it in this repository: update-alertmanager.py. This script will check the current installed version and it will compare it with the latest stable release version on Github. If there is a new version available, it will prompt you to update it. This script require a somewhat recent version of Python 3 which should already be installed on your system.</p> <p>You can easily download this script with:</p> <pre><code>$ wget https://raw.githubusercontent.com/eth-educators/ethstaker-guides/main/docs/scripts/update-alertmanager.py\n</code></pre> <p>To run the Alertmanager update script, use this command:</p> <pre><code>$ python3 update-alertmanager.py\n</code></pre>"},{"location":"alerting/#support","title":"Support","text":"<p>If you have any question or if you need additional support, make sure to get in touch with the ethstaker community on:</p> <ul> <li>Discord: dsc.gg/ethstaker</li> <li>Reddit: reddit.com/r/ethstaker</li> </ul>"},{"location":"alerting/#credits","title":"Credits","text":"<p>Based on Somer Esat's guide.</p>"},{"location":"hoodi-node-alt/","title":"Guide on how to setup a Hoodi testnet node (Reth/Lodestar)","text":"<p>Hoodi is a new Ethereum testnet meant to replace Holesky as a staking, infrastructure and protocol-developer testnet.</p> <p>This guide is meant for people with little or some experience in running Ethereum clients and using the command-line interface (CLI). It will show you step by step how to setup your machine to join the Hoodi testnet by giving you the instructions to install and configure all the tools needed. It will assume you are using a modern linux distribution with systemd and APT (like Ubuntu 24.04 or Debian 12) on a modern x86 CPU (Intel, AMD). A clean install of your operating system on a dedicated machine or a virtual machine before proceeding is preferable.</p>"},{"location":"hoodi-node-alt/#overview","title":"Overview","text":"<p>We will use the latest version for Reth and the latest version for Lodestar. We will configure them to connect to the Hoodi testnet.</p>"},{"location":"hoodi-node-alt/#executing-the-commands","title":"Executing the commands","text":"<p>Almost all of these commands will be performed in a terminal. Start your Terminal application. Any line that starts with the dollar sign (<code>$</code>) is a command that need to be executed in your terminal. Do not input the dollar sign (<code>$</code>) in your terminal, only the text that comes after that.</p> <p>Executing a command with <code>sudo</code> will occasionally ask you for your password. Make sure to enter your account password correctly. You can execute the command again if you fail to enter the correct password after a few attempts.</p>"},{"location":"hoodi-node-alt/#installing-prerequisites","title":"Installing Prerequisites","text":"<p>Make sure we have fully updated packages first.</p> <pre><code>$ sudo apt -y update\n$ sudo apt -y upgrade\n</code></pre> <p>Install prerequisites commonly available.</p> <pre><code>$ sudo apt -y install wget curl ccze\n</code></pre>"},{"location":"hoodi-node-alt/#installing-reth","title":"Installing Reth","text":"<p>Download the latest release version for Reth and extract it. If the latest version is more recent than what is used here, use that version and adjust for the new URL and archive name. Make sure to use the linux x86_64 version.</p> <pre><code>$ cd ~\n$ wget https://github.com/paradigmxyz/reth/releases/download/v1.3.12/reth-v1.3.12-x86_64-unknown-linux-gnu.tar.gz\n$ tar xvf reth-v1.3.12-x86_64-unknown-linux-gnu.tar.gz\n$ rm reth-v1.3.12-x86_64-unknown-linux-gnu.tar.gz\n</code></pre> <p>Install this Reth version globally.</p> <pre><code>$ sudo cp ~/reth /usr/local/bin\n$ rm ~/reth\n</code></pre>"},{"location":"hoodi-node-alt/#installing-lodestar","title":"Installing Lodestar","text":"<p>Download the latest release version for Lodestar and extract it. If the latest version is more recent than what is used here, use that version and adjust for the new URL and archive name. Make sure to use the linux amd64 version.</p> <pre><code>$ cd ~\n$ wget https://github.com/ChainSafe/lodestar/releases/download/v1.29.0-rc.2/lodestar-v1.29.0-rc.2-linux-amd64.tar.gz\n$ tar xvf lodestar-v1.29.0-rc.2-linux-amd64.tar.gz\n$ rm lodestar-v1.29.0-rc.2-linux-amd64.tar.gz\n</code></pre> <p>Install this Lodestar version globally.</p> <pre><code>$ sudo cp ~/lodestar /usr/local/bin\n$ rm ~/lodestar\n</code></pre>"},{"location":"hoodi-node-alt/#creating-the-jwt-token-file","title":"Creating the JWT token file","text":"<p>Create a JWT token file in a neutral location and make it readable to everyone. We will use the <code>/var/lib/ethereum/jwttoken</code> location to store the JWT token file.</p> <pre><code>$ sudo mkdir -p /var/lib/ethereum\n$ openssl rand -hex 32 | tr -d \"\\n\" | sudo tee /var/lib/ethereum/jwttoken\n$ sudo chmod +r /var/lib/ethereum/jwttoken\n</code></pre>"},{"location":"hoodi-node-alt/#configuring-your-reth-node","title":"Configuring your Reth node","text":"<p>Create a dedicated user for running Reth, create a directory for holding the data and assign the proper permissions.</p> <pre><code>$ sudo useradd --no-create-home --shell /bin/false reth\n$ sudo mkdir -p /var/lib/reth/logs\n$ sudo chown -R reth:reth /var/lib/reth\n</code></pre> <p>Create a systemd service config file to configure the Reth node service.</p> <pre><code>$ sudo nano /etc/systemd/system/reth.service\n</code></pre> <p>Paste the following service configuration into the file. Exit and save once done (<code>Ctrl</code> + <code>X</code>, <code>Y</code>, <code>Enter</code>).</p> <pre><code>[Unit]\nDescription=Reth Execution Client (Hoodi)\nAfter=network.target\nWants=network.target\n\n[Service]\nUser=reth\nGroup=reth\nType=simple\nRestart=always\nRestartSec=5\nTimeoutStopSec=180\nExecStart=reth node \\\n  --full \\\n  --chain hoodi \\\n  --datadir /var/lib/reth \\\n  --log.file.directory /var/lib/reth/logs \\\n  --metrics 6061 \\\n  --authrpc.jwtsecret /var/lib/ethereum/jwttoken\n\n[Install]\nWantedBy=default.target\n</code></pre> <p>Reload systemd to reflect the changes and start the service. Check status to make sure it\u2019s running correctly.</p> <pre><code>$ sudo systemctl daemon-reload\n$ sudo systemctl start reth.service\n$ sudo systemctl status reth.service\n</code></pre> <p>It should say active (running) in green text. If not then go back and repeat the steps to fix the problem. Press Q to quit (will not affect the reth service).</p> <p>Enable the reth service to automatically start on reboot.</p> <pre><code>$ sudo systemctl enable reth.service\n</code></pre> <p>You can watch the live messages from your Reth node logs using this command. Make sure nothing suspicious shows up in your logs.</p> <pre><code>$ sudo journalctl -f -u reth.service -o cat | ccze -A\n</code></pre> <p>Press <code>Ctrl</code> + <code>C</code> to stop showing those messages.</p>"},{"location":"hoodi-node-alt/#configuring-your-lodestar-beacon-node","title":"Configuring your Lodestar beacon node","text":"<p>Create a dedicated user for running the Lodestar beacon node, create a directory for holding the data, copy testnet files and assign the proper permissions.</p> <pre><code>$ sudo useradd --no-create-home --shell /bin/false lodestarbeacon\n$ sudo mkdir -p /var/lib/lodestar\n$ sudo chown -R lodestarbeacon:lodestarbeacon /var/lib/lodestar\n</code></pre> <p>Create a systemd service config file to configure the Lodestar beacon node service.</p> <pre><code>$ sudo nano /etc/systemd/system/lodestarbeacon.service\n</code></pre> <p>Paste the following service configuration into the file. Exit and save once done (<code>Ctrl</code> + <code>X</code>, <code>Y</code>, <code>Enter</code>).</p> <pre><code>[Unit]\nDescription=Lodestar Beacon Node (Hoodi)\nWants=network-online.target\nAfter=network-online.target\n\n[Service]\nType=simple\nUser=lodestarbeacon\nGroup=lodestarbeacon\nRestart=always\nRestartSec=5\nExecStart=lodestar beacon \\\n    --network hoodi \\\n    --dataDir /var/lib/lodestar \\\n    --checkpointSyncUrl https://hoodi.beaconstate.ethstaker.cc \\\n    --metrics \\\n    --metrics.port 6071 \\\n    --jwtSecret /var/lib/ethereum/jwttoken\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <p>Reload systemd to reflect the changes and start the service. Check status to make sure it\u2019s running correctly.</p> <pre><code>$ sudo systemctl daemon-reload\n$ sudo systemctl start lodestarbeacon.service\n$ sudo systemctl status lodestarbeacon.service\n</code></pre> <p>It should say active (running) in green text. If not then go back and repeat the steps to fix the problem. Press Q to quit (will not affect the Lodestar beacon node service).</p> <p>Enable the Lodestar beacon node service to automatically start on reboot.</p> <pre><code>$ sudo systemctl enable lodestarbeacon.service\n</code></pre> <p>You can watch the live messages from your Lodestar beacon node logs using this command. Make sure nothing suspicious shows up in your logs.</p> <pre><code>$ sudo journalctl -f -u lodestarbeacon.service -o cat | ccze -A\n</code></pre> <p>Press <code>Ctrl</code> + <code>C</code> to stop showing those messages.</p>"},{"location":"hoodi-node-alt/#trying-the-hoodi-testnet","title":"Trying the Hoodi testnet","text":""},{"location":"hoodi-node-alt/#requesting-testnet-funds","title":"Requesting testnet funds","text":"<p>Requesting or obtaining enough Hoodi ETH to perform your validator deposit can be challenging. We suggest you use the EthStaker #cheap-hoodi-validator free process. Join the EthStaker Discord server and use the <code>/cheap-hoodi-deposit</code> slash command (start typing the command and it will show up above your input box). From there, follow the instructions from the bot. As an alternative, you can try obtaining 32 Hoodi ETH from this great faucet on https://hoodi-faucet.pk910.de/ . Hoodi ETH was also distributed to many people who previously interacted with the Holesky network. You might already have some Hoodi ETH in your wallet if you used Holesky so check first.</p>"},{"location":"hoodi-node-alt/#adding-a-validator","title":"Adding a validator","text":""},{"location":"hoodi-node-alt/#creating-your-validator-keys-and-performing-the-deposit","title":"Creating your validator keys and performing the deposit","text":"<p>There are 2 great tools to create your validator keys:</p> <ul> <li>GUI based: Wagyu Key Gen</li> <li>CLI based: ethstaker-deposit-cli</li> </ul> <p>If you choose the Wagyu Key Gen application, make sure to select the Hoodi network and follow the instructions provided. If you are using the #cheap-hoodi-validator process, you will need to use <code>0x4D496CcC28058B1D74B7a19541663E21154f9c84</code> as your withdrawal address. This is only required for that process. When on Mainnet, you should use a withdrawal address you control if you want to use one.</p> <p>If you choose the ethstaker-deposit-cli application, here is how to create your validator keys. Make sure to replace the <code>0x4D496CcC28058B1D74B7a19541663E21154f9c84</code> withdrawal address with your own address that you control if you need or want to:</p> <pre><code>$ cd ~\n$ wget https://github.com/eth-educators/ethstaker-deposit-cli/releases/download/v1.1.0/ethstaker_deposit-cli-08f1e66-linux-amd64.tar.gz\n$ tar xvf ethstaker_deposit-cli-08f1e66-linux-amd64.tar.gz\n$ rm ethstaker_deposit-cli-08f1e66-linux-amd64.tar.gz\n$ cd ethstaker_deposit-cli-08f1e66-linux-amd64/\n$ ./deposit new-mnemonic --num_validators 1 --chain hoodi --execution_address 0x4D496CcC28058B1D74B7a19541663E21154f9c84\n$ ls -d $PWD/validator_keys/*\n</code></pre> <p>Make sure to store your keystore password and your mnemonic somewhere safe. You should end up with a deposit file (starts with <code>deposit_data-</code> and ends with <code>.json</code>) and one or more keystore files (starts with <code>keystore-</code> and ends with <code>.json</code>), 1 per validator. Copy them around if needed. Make sure your deposit file and your keystore files are in a known and accessible location on your machine.</p> <p>Next we will need to perform your deposit. If you used the #cheap-hoodi-validator process, you can perform your deposit on https://cheap.hoodi.launchpad.ethstaker.cc/ . If you managed to obtained 32 Hoodi ETH, you can use the official Hoodi launchpad on https://hoodi.launchpad.ethereum.org/ .</p>"},{"location":"hoodi-node-alt/#configuring-your-lodestar-validator-client","title":"Configuring your Lodestar validator client","text":"<p>Create a dedicated user for running the Lodestar validator client, create a directory for holding the data and assign the proper permissions.</p> <pre><code>$ sudo useradd --no-create-home --shell /bin/false lodestarvalidator\n$ sudo mkdir -p /var/lib/lodestar/validators\n$ sudo chown -R lodestarvalidator:lodestarvalidator /var/lib/lodestar/validators\n$ sudo chmod 700 /var/lib/lodestar/validators\n</code></pre> <p>Import your keystore that includes your validator key for the Lodestar validator client. Running the first command will prompt you for that keystore password. Make sure to enter it correctly and avoid leaving it blank. Make sure to replace <code>/path/to/keystores</code> with the actual path to your keystores created in the previous step.</p> <pre><code>$ sudo lodestar validator import \\\n    --importKeystores /path/to/keystores \\\n    --dataDir /var/lib/lodestar/validators \\\n    --network hoodi\n$ sudo chown -R lodestarvalidator:lodestarvalidator /var/lib/lodestar/validators\n</code></pre> <p>Create a systemd service config file to configure the Lodestar validator client service.</p> <pre><code>$ sudo nano /etc/systemd/system/lodestarvalidator.service\n</code></pre> <p>Paste the following service configuration into the file. Exit and save once done (<code>Ctrl</code> + <code>X</code>, <code>Y</code>, <code>Enter</code>). Make sure to replace the <code>0x0000000000000000000000000000000000000000</code> address with your own Ethereum address that you control where you want to receive the transaction tips.</p> <pre><code>[Unit]\nDescription=Lodestar Validator Client (Hoodi)\nWants=network-online.target\nAfter=network-online.target\n\n[Service]\nUser=lodestarvalidator\nGroup=lodestarvalidator\nType=simple\nRestart=always\nRestartSec=5\nExecStart=lodestar validator \\\n    --network hoodi \\\n    --dataDir /var/lib/lodestar/validators \\\n    --graffiti EthStaker \\\n    --metrics true \\\n    --suggestedFeeRecipient 0x0000000000000000000000000000000000000000\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <p>Reload systemd to reflect the changes and start the service. Check status to make sure it\u2019s running correctly.</p> <pre><code>$ sudo systemctl daemon-reload\n$ sudo systemctl start lodestarvalidator.service\n$ sudo systemctl status lodestarvalidator.service\n</code></pre> <p>It should say active (running) in green text. If not then go back and repeat the steps to fix the problem. Press Q to quit (will not affect the Lodestar validator client service).</p> <p>Enable the Lodestar validator client service to automatically start on reboot.</p> <pre><code>$ sudo systemctl enable lodestarvalidator.service\n</code></pre> <p>You can watch the live messages from your Lodestar validator client logs using this command.</p> <pre><code>$ sudo journalctl -f -u lodestarvalidator.service -o cat | ccze -A\n</code></pre> <p>Press <code>Ctrl</code> + <code>C</code> to stop showing those messages.</p>"},{"location":"hoodi-node-alt/#support","title":"Support","text":"<p>If you have any question or if you need additional support, make sure to get in touch with the EthStaker community on:</p> <ul> <li>Discord: dsc.gg/ethstaker</li> <li>Reddit: reddit.com/r/ethstaker</li> </ul>"},{"location":"hoodi-node-alt/#credits","title":"Credits","text":"<p>Based on Somer Esat's guide.</p>"},{"location":"hoodi-node/","title":"Guide on how to setup a Hoodi testnet node (Nethermind/Lighthouse)","text":"<p>Hoodi is a new Ethereum testnet meant to replace Holesky as a staking, infrastructure and protocol-developer testnet.</p> <p>This guide is meant for people with little or some experience in running Ethereum clients and using the command-line interface (CLI). It will show you step by step how to setup your machine to join the Hoodi testnet by giving you the instructions to install and configure all the tools needed. It will assume you are using a modern linux distribution with systemd and APT (like Ubuntu 24.04 or Debian 12) on a modern x86 CPU (Intel, AMD). A clean install of your operating system on a dedicated machine or a virtual machine before proceeding is preferable.</p>"},{"location":"hoodi-node/#overview","title":"Overview","text":"<p>We will use the latest version for Nethermind and the latest version for Lighthouse. We will configure them to connect to the Hoodi testnet.</p>"},{"location":"hoodi-node/#executing-the-commands","title":"Executing the commands","text":"<p>Almost all of these commands will be performed in a terminal. Start your Terminal application. Any line that starts with the dollar sign (<code>$</code>) is a command that need to be executed in your terminal. Do not input the dollar sign (<code>$</code>) in your terminal, only the text that comes after that.</p> <p>Executing a command with <code>sudo</code> will occasionally ask you for your password. Make sure to enter your account password correctly. You can execute the command again if you fail to enter the correct password after a few attempts.</p>"},{"location":"hoodi-node/#installing-prerequisites","title":"Installing Prerequisites","text":"<p>Make sure we have fully updated packages first.</p> <pre><code>$ sudo apt -y update\n$ sudo apt -y upgrade\n</code></pre> <p>Install prerequisites commonly available.</p> <pre><code>$ sudo apt -y install wget curl ccze unzip\n</code></pre>"},{"location":"hoodi-node/#installing-nethermind","title":"Installing Nethermind","text":"<p>Download the latest release version for Nethermind and extract it. If the latest version is more recent than what is used here, use that version and adjust for the new URL and archive name. Make sure to use the linux-x64 version.</p> <pre><code>$ cd ~\n$ wget https://github.com/NethermindEth/nethermind/releases/download/1.31.6/nethermind-1.31.6-4e68f8ee-linux-x64.zip\n$ sudo mkdir -p /usr/share/nethermind\n$ sudo unzip nethermind-1.31.6-4e68f8ee-linux-x64.zip -d /usr/share/nethermind\n$ rm nethermind-1.31.6-4e68f8ee-linux-x64.zip\n</code></pre>"},{"location":"hoodi-node/#installing-lighthouse","title":"Installing Lighthouse","text":"<p>Download this specific version for Lighthouse and extract it. If the latest version is more recent and supports the Hoodi network, use that version and adjust for the new URL and archive name. Make sure to use the linux x86_64 version.</p> <pre><code>$ cd ~\n$ wget https://github.com/sigp/lighthouse/releases/download/v7.0.0-beta.7/lighthouse-v7.0.0-beta.7-x86_64-unknown-linux-gnu.tar.gz\n$ tar xvf lighthouse-v7.0.0-beta.7-x86_64-unknown-linux-gnu.tar.gz\n$ rm lighthouse-v7.0.0-beta.7-x86_64-unknown-linux-gnu.tar.gz\n</code></pre> <p>Install this Lighthouse version globally.</p> <pre><code>$ sudo cp ~/lighthouse /usr/local/bin\n$ rm ~/lighthouse\n</code></pre>"},{"location":"hoodi-node/#creating-the-jwt-token-file","title":"Creating the JWT token file","text":"<p>Create a JWT token file in a neutral location and make it readable to everyone. We will use the <code>/var/lib/ethereum/jwttoken</code> location to store the JWT token file.</p> <pre><code>$ sudo mkdir -p /var/lib/ethereum\n$ openssl rand -hex 32 | tr -d \"\\n\" | sudo tee /var/lib/ethereum/jwttoken\n$ sudo chmod +r /var/lib/ethereum/jwttoken\n</code></pre>"},{"location":"hoodi-node/#configuring-your-nethermind-node","title":"Configuring your Nethermind node","text":"<p>Create a dedicated user for running Nethermind, create a directory for holding the data and assign the proper permissions.</p> <pre><code>$ sudo useradd --no-create-home --shell /bin/false nethermind\n$ sudo mkdir -p /var/lib/nethermind\n$ sudo chown -R nethermind:nethermind /var/lib/nethermind\n</code></pre> <p>Create a systemd service config file to configure the Nethermind node service.</p> <pre><code>$ sudo nano /etc/systemd/system/nethermind.service\n</code></pre> <p>Paste the following service configuration into the file. Exit and save once done (<code>Ctrl</code> + <code>X</code>, <code>Y</code>, <code>Enter</code>).</p> <pre><code>[Unit]\nDescription=Nethermind Execution Client (Hoodi)\nAfter=network.target\nWants=network.target\n\n[Service]\nUser=nethermind\nGroup=nethermind\nType=simple\nRestart=always\nRestartSec=5\nTimeoutStopSec=180\nWorkingDirectory=/var/lib/nethermind\nEnvironment=\"DOTNET_BUNDLE_EXTRACT_BASE_DIR=/var/lib/nethermind\"\nExecStart=/usr/share/nethermind/nethermind \\\n  --config hoodi \\\n  --datadir /var/lib/nethermind \\\n  --Metrics.Enabled true \\\n  --Metrics.ExposePort 6061 \\\n  --Sync.SnapSync true \\\n  --JsonRpc.JwtSecretFile /var/lib/ethereum/jwttoken \\\n  --JsonRpc.Enabled true \\\n  --HealthChecks.Enabled true \\\n  --JsonRpc.EnabledModules='[Eth, Subscribe, Trace, TxPool, Web3, Personal, Proof, Net, Parity, Health, Rpc, Admin]' \\\n  --JsonRpc.AdditionalRpcUrls=http://127.0.0.1:8555|http|admin \\\n  --JsonRpc.EnginePort 8551\n\n[Install]\nWantedBy=default.target\n</code></pre> <p>Reload systemd to reflect the changes and start the service. Check status to make sure it\u2019s running correctly.</p> <pre><code>$ sudo systemctl daemon-reload\n$ sudo systemctl start nethermind.service\n$ sudo systemctl status nethermind.service\n</code></pre> <p>It should say active (running) in green text. If not then go back and repeat the steps to fix the problem. Press Q to quit (will not affect the nethermind service).</p> <p>Enable the nethermind service to automatically start on reboot.</p> <pre><code>$ sudo systemctl enable nethermind.service\n</code></pre> <p>You can watch the live messages from your Nethermind node logs using this command. Make sure nothing suspicious shows up in your logs.</p> <pre><code>$ sudo journalctl -f -u nethermind.service -o cat | ccze -A\n</code></pre> <p>Press <code>Ctrl</code> + <code>C</code> to stop showing those messages.</p>"},{"location":"hoodi-node/#configuring-your-lighthouse-beacon-node","title":"Configuring your Lighthouse beacon node","text":"<p>Create a dedicated user for running the Lighthouse beacon node, create a directory for holding the data, copy testnet files and assign the proper permissions.</p> <pre><code>$ sudo useradd --no-create-home --shell /bin/false lighthousebeacon\n$ sudo mkdir -p /var/lib/lighthouse\n$ sudo chown -R lighthousebeacon:lighthousebeacon /var/lib/lighthouse\n</code></pre> <p>Create a systemd service config file to configure the Lighthouse beacon node service.</p> <pre><code>$ sudo nano /etc/systemd/system/lighthousebeacon.service\n</code></pre> <p>Paste the following service configuration into the file. Exit and save once done (<code>Ctrl</code> + <code>X</code>, <code>Y</code>, <code>Enter</code>).</p> <pre><code>[Unit]\nDescription=Lighthouse Beacon Node (Hoodi)\nWants=network-online.target\nAfter=network-online.target\n\n[Service]\nType=simple\nUser=lighthousebeacon\nGroup=lighthousebeacon\nRestart=always\nRestartSec=5\nExecStart=/usr/local/bin/lighthouse bn \\\n    --network hoodi \\\n    --datadir /var/lib/lighthouse \\\n    --http \\\n    --execution-endpoint http://127.0.0.1:8551 \\\n    --checkpoint-sync-url https://hoodi.beaconstate.ethstaker.cc \\\n    --metrics \\\n    --execution-jwt /var/lib/ethereum/jwttoken \\\n    --validator-monitor-auto \\\n    --boot-nodes=enr:-OS4QMJGE13xEROqvKN1xnnt7U-noc51VXyM6wFMuL9LMhQDfo1p1dF_zFdS4OsnXz_vIYk-nQWnqJMWRDKvkSK6_CwDh2F0dG5ldHOIAAAAADAAAACGY2xpZW502IpMaWdodGhvdXNljDcuMC4wLWJldGEuM4RldGgykNLxmX9gAAkQAAgAAAAAAACCaWSCdjSCaXCEhse4F4RxdWljgiMqiXNlY3AyNTZrMaECef77P8k5l3PC_raLw42OAzdXfxeQ-58BJriNaqiRGJSIc3luY25ldHMAg3RjcIIjKIN1ZHCCIyg,enr:-LK4QDwhXMitMbC8xRiNL-XGMhRyMSOnxej-zGifjv9Nm5G8EF285phTU-CAsMHRRefZimNI7eNpAluijMQP7NDC8kEMh2F0dG5ldHOIAAAAAAAABgCEZXRoMpDS8Zl_YAAJEAAIAAAAAAAAgmlkgnY0gmlwhAOIT_SJc2VjcDI1NmsxoQMoHWNL4MAvh6YpQeM2SUjhUrLIPsAVPB8nyxbmckC6KIN0Y3CCIyiDdWRwgiMo,enr:-LK4QPYl2HnMPQ7b1es6Nf_tFYkyya5bj9IqAKOEj2cmoqVkN8ANbJJJK40MX4kciL7pZszPHw6vLNyeC-O3HUrLQv8Mh2F0dG5ldHOIAAAAAAAAAMCEZXRoMpDS8Zl_YAAJEAAIAAAAAAAAgmlkgnY0gmlwhAMYRG-Jc2VjcDI1NmsxoQPQ35tjr6q1qUqwAnegQmYQyfqxC_6437CObkZneI9n34N0Y3CCIyiDdWRwgiMo\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <p>Reload systemd to reflect the changes and start the service. Check status to make sure it\u2019s running correctly.</p> <pre><code>$ sudo systemctl daemon-reload\n$ sudo systemctl start lighthousebeacon.service\n$ sudo systemctl status lighthousebeacon.service\n</code></pre> <p>It should say active (running) in green text. If not then go back and repeat the steps to fix the problem. Press Q to quit (will not affect the Lighthouse beacon node service).</p> <p>Enable the Lighthouse beacon node service to automatically start on reboot.</p> <pre><code>$ sudo systemctl enable lighthousebeacon.service\n</code></pre> <p>You can watch the live messages from your Lighthouse beacon node logs using this command. Make sure nothing suspicious shows up in your logs.</p> <pre><code>$ sudo journalctl -f -u lighthousebeacon.service -o cat | ccze -A\n</code></pre> <p>Press <code>Ctrl</code> + <code>C</code> to stop showing those messages.</p>"},{"location":"hoodi-node/#trying-the-hoodi-testnet","title":"Trying the Hoodi testnet","text":""},{"location":"hoodi-node/#requesting-testnet-funds","title":"Requesting testnet funds","text":"<p>Requesting or obtaining enough Hoodi ETH to perform your validator deposit can be challenging. We suggest you use the EthStaker #cheap-hoodi-validator free process. Join the EthStaker Discord server and use the <code>/cheap-hoodi-deposit</code> slash command (start typing the command and it will show up above your input box). From there, follow the instructions from the bot. As an alternative, you can try obtaining 32 Hoodi ETH from this great faucet on https://hoodi-faucet.pk910.de/ . Hoodi ETH was also distributed to many people who previously interacted with the Holesky network. You might already have some Hoodi ETH in your wallet if you used Holesky so check first.</p>"},{"location":"hoodi-node/#adding-a-validator","title":"Adding a validator","text":""},{"location":"hoodi-node/#creating-your-validator-keys-and-performing-the-deposit","title":"Creating your validator keys and performing the deposit","text":"<p>There are 2 great tools to create your validator keys:</p> <ul> <li>GUI based: Wagyu Key Gen</li> <li>CLI based: ethstaker-deposit-cli</li> </ul> <p>If you choose the Wagyu Key Gen application, make sure to select the Hoodi network and follow the instructions provided. If you are using the #cheap-hoodi-validator process, you will need to use <code>0x4D496CcC28058B1D74B7a19541663E21154f9c84</code> as your withdrawal address. This is only required for that process. When on Mainnet, you should use a withdrawal address you control if you want to use one.</p> <p>If you choose the ethstaker-deposit-cli application, here is how to create your validator keys. Make sure to replace the <code>0x4D496CcC28058B1D74B7a19541663E21154f9c84</code> withdrawal address with your own address that you control if you need or want to:</p> <pre><code>$ cd ~\n$ wget https://github.com/eth-educators/ethstaker-deposit-cli/releases/download/v1.1.0/ethstaker_deposit-cli-08f1e66-linux-amd64.tar.gz\n$ tar xvf ethstaker_deposit-cli-08f1e66-linux-amd64.tar.gz\n$ rm ethstaker_deposit-cli-08f1e66-linux-amd64.tar.gz\n$ cd ethstaker_deposit-cli-08f1e66-linux-amd64/\n$ ./deposit new-mnemonic --num_validators 1 --chain hoodi --execution_address 0x4D496CcC28058B1D74B7a19541663E21154f9c84\n$ ls -d $PWD/validator_keys/*\n</code></pre> <p>Make sure to store your keystore password and your mnemonic somewhere safe. You should end up with a deposit file (starts with <code>deposit_data-</code> and ends with <code>.json</code>) and one or more keystore files (starts with <code>keystore-</code> and ends with <code>.json</code>), 1 per validator. Copy them around if needed. Make sure your deposit file and your keystore files are in a known and accessible location on your machine.</p> <p>Next we will need to perform your deposit. If you used the #cheap-hoodi-validator process, you can perform your deposit on https://cheap.hoodi.launchpad.ethstaker.cc/ . If you managed to obtained 32 Hoodi ETH, you can use the official Hoodi launchpad on https://hoodi.launchpad.ethereum.org/ .</p>"},{"location":"hoodi-node/#configuring-your-lighthouse-validator-client","title":"Configuring your Lighthouse validator client","text":"<p>Create a dedicated user for running the Lighthouse validator client, create a directory for holding the data and assign the proper permissions.</p> <pre><code>$ sudo useradd --no-create-home --shell /bin/false lighthousevalidator\n$ sudo mkdir -p /var/lib/lighthouse/validators\n$ sudo chown -R lighthousevalidator:lighthousevalidator /var/lib/lighthouse/validators\n$ sudo chmod 700 /var/lib/lighthouse/validators\n</code></pre> <p>Import your keystore that includes your validator key for the Lighthouse validator client. Running the first command will prompt you for that keystore password. Make sure to enter it correctly and avoid leaving it blank. Make sure to replace <code>/path/to/keystores</code> with the actual path to your keystores created in the previous step.</p> <pre><code>$ sudo /usr/local/bin/lighthouse account validator import \\\n    --directory /path/to/keystores \\\n    --datadir /var/lib/lighthouse \\\n    --network hoodi\n$ sudo chown -R lighthousevalidator:lighthousevalidator /var/lib/lighthouse/validators\n</code></pre> <p>Create a systemd service config file to configure the Lighthouse validator client service.</p> <pre><code>$ sudo nano /etc/systemd/system/lighthousevalidator.service\n</code></pre> <p>Paste the following service configuration into the file. Exit and save once done (<code>Ctrl</code> + <code>X</code>, <code>Y</code>, <code>Enter</code>). Make sure to replace the <code>0x0000000000000000000000000000000000000000</code> address with your own Ethereum address that you control where you want to receive the transaction tips.</p> <pre><code>[Unit]\nDescription=Lighthouse Validator Client (Hoodi)\nWants=network-online.target\nAfter=network-online.target\n\n[Service]\nUser=lighthousevalidator\nGroup=lighthousevalidator\nType=simple\nRestart=always\nRestartSec=5\nExecStart=/usr/local/bin/lighthouse vc \\\n    --network hoodi \\\n    --datadir /var/lib/lighthouse \\\n    --graffiti EthStaker \\\n    --metrics \\\n    --suggested-fee-recipient 0x0000000000000000000000000000000000000000\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <p>Reload systemd to reflect the changes and start the service. Check status to make sure it\u2019s running correctly.</p> <pre><code>$ sudo systemctl daemon-reload\n$ sudo systemctl start lighthousevalidator.service\n$ sudo systemctl status lighthousevalidator.service\n</code></pre> <p>It should say active (running) in green text. If not then go back and repeat the steps to fix the problem. Press Q to quit (will not affect the Lighthouse validator client service).</p> <p>Enable the Lighthouse validator client service to automatically start on reboot.</p> <pre><code>$ sudo systemctl enable lighthousevalidator.service\n</code></pre> <p>You can watch the live messages from your Lighthouse validator client logs using this command.</p> <pre><code>$ sudo journalctl -f -u lighthousevalidator.service -o cat | ccze -A\n</code></pre> <p>Press <code>Ctrl</code> + <code>C</code> to stop showing those messages.</p>"},{"location":"hoodi-node/#support","title":"Support","text":"<p>If you have any question or if you need additional support, make sure to get in touch with the EthStaker community on:</p> <ul> <li>Discord: dsc.gg/ethstaker</li> <li>Reddit: reddit.com/r/ethstaker</li> </ul>"},{"location":"hoodi-node/#credits","title":"Credits","text":"<p>Based on Somer Esat's guide.</p>"},{"location":"migrating-to-a-larger-disk/","title":"Guide on How to Migrate to a Larger Disk for an Ethereum Validator","text":"<p>Disk usage is a constant concern for stakers and node operators. The blockchain data keeps growing each day with new transactions and states to store. Stakers and node operators are mainly those responsible for storing this data. During the life of a staker or a node operator, it is likely that you will need more space for this concern. Migrating to a larger disk might become the last solution for some.</p> <p>This guide is meant for people with little or no experience migrating to a larger disk. It will show you step by step how to check your disk usage, which solutions are possible to manage disk usage, which tools you will need to migrate to a larger disk, and the process of migrating to a larger disk. It will assume you are using a modern Linux distribution to run your staker or node operating system. Many of these instructions and steps are possible on Windows or macOS, but this is out of scope for this guide.</p> <p>A video tutorial of this guide can be seen on https://youtu.be/xWyf84Syq7o .</p> <p>If you know what you are doing, you can simply read the Migrating to a Larger Disk section.</p>"},{"location":"migrating-to-a-larger-disk/#current-expected-disk-usage","title":"Current Expected Disk Usage","text":"<p>On Mainnet, you should expect the current disk usage to be around 1.1 TB to 1.7 TB, depending on which client and configuration you are using, excluding archive nodes. This should be accurate as of today, November 2024. Any serious staker or node operator should be using a good 2+ TB SSD. If you are building a new machine, you should consider buying or starting with a good 4 TB SSD to avoid having to perform this kind of maintenance in the short or medium term.</p>"},{"location":"migrating-to-a-larger-disk/#solutions-to-check-before-buying-a-new-disk-and-migrating","title":"Solutions to Check Before Buying a New Disk and Migrating","text":"<p>I recommend running ncdu as described below as the first step to figure out exactly which application or directory is consuming the most disk space. A common low-hanging fruit is to resync your consensus client with a checkpoint sync endpoint especially if it has been a while since your last resync. If your consensus client is using way more than around 110 GB of disk space, you will likely benefit from this resync. If your execution client is using way more than 1.1 TB of disk space, there are ways to improve it described below.</p>"},{"location":"migrating-to-a-larger-disk/#checking-your-current-disk-usage","title":"Checking Your Current Disk Usage","text":"<p>On Linux, you can run <code>df -h</code> to find out more about filesystem disk usage. It will return something like this:</p> <pre><code>Filesystem                         Size  Used Avail Use% Mounted on\ntmpfs                              1.6G  2.6M  1.6G   1% /run\n/dev/mapper/ubuntu--vg-ubuntu--lv  913G  378G  489G  44% /\ntmpfs                              7.6G     0  7.6G   0% /dev/shm\ntmpfs                              5.0M     0  5.0M   0% /run/lock\n/dev/nvme0n1p2                     2.0G  375M  1.5G  21% /boot\n/dev/nvme0n1p1                     1.1G  6.4M  1.1G   1% /boot/efi\ntmpfs                              1.6G  4.0K  1.6G   1% /run/user/1000\n</code></pre> <p>In this example, you can see the root mount <code>/</code> is the main one with 913 GB of total disk space, of which 378 GB is used, leaving 489 GB of free disk space.</p> <p>On Linux, you can run <code>sudo fdisk -l</code> to find out more about your disks and partitions. It will return something like:</p> <pre><code>Disk /dev/nvme0n1: 931.51 GiB, 1000204886016 bytes, 1953525168 sectors\nDisk model: WDS100T3X0C-00SJG0\nUnits: sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 512 bytes\nDisklabel type: gpt\nDisk identifier: XXX\n\nDevice           Start        End    Sectors   Size Type\n/dev/nvme0n1p1    2048    2203647    2201600     1G EFI System\n/dev/nvme0n1p2 2203648    6397951    4194304     2G Linux filesystem\n/dev/nvme0n1p3 6397952 1953521663 1947123712 928.5G Linux filesystem\n\n\nDisk /dev/mapper/ubuntu--vg-ubuntu--lv: 928.46 GiB, 996923146240 bytes, 1947115520 sectors\nUnits: sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 512 bytes\n</code></pre> <p>In this example, you can see a single SSD on <code>/dev/nvme0n1</code> with a 931.51 GiB size and a few partitions.</p> <p>On Linux, if you have ncdu installed (it can be installed with <code>sudo apt install ncdu</code> on any Debian-related distro), you can run <code>sudo ncdu -x -q /</code> to get some great insights into which directory and which application is using the most disk space. You can interactively browse into the larger directories to figure out which subdirectories are using most of your disk space. It will return something like:</p> <pre><code>ncdu 1.15.1 ~ Use the arrow keys to navigate, press ? for help\n--- / --------------------------------------------------------------------------\n    1.0 TiB [##########] /var\n    8.0 GiB [          ] /home\n    4.0 GiB [          ]  swap.img\n    3.8 GiB [          ] /usr\n    6.2 MiB [          ] /etc\n    2.3 MiB [          ] /root\n  104.0 KiB [          ] /tmp\n   36.0 KiB [          ] /snap\ne  16.0 KiB [          ] /lost+found\ne   4.0 KiB [          ] /srv\ne   4.0 KiB [          ] /opt\ne   4.0 KiB [          ] /mnt\ne   4.0 KiB [          ] /media\n@   0.0   B [          ]  libx32\n@   0.0   B [          ]  lib64\n@   0.0   B [          ]  lib32\n@   0.0   B [          ]  sbin\n@   0.0   B [          ]  lib\n@   0.0   B [          ]  bin\n&gt;   0.0   B [          ] /sys\n&gt;   0.0   B [          ] /run\n</code></pre> <p>Browsing into <code>/var/lib</code> will reveal how much disk space subdirectories in that directory are using:</p> <pre><code>ncdu 1.15.1 ~ Use the arrow keys to navigate, press ? for help\n--- /var/lib -------------------------------------------------------------------\n                         /..\n  929.4 GiB [##########] /goethereum\n   64.7 GiB [          ] /lighthouse\n    1.1 GiB [          ] /prometheus\n    1.0 GiB [          ] /snapd\n  164.6 MiB [          ] /apt\n   32.9 MiB [          ] /dpkg\n    3.2 MiB [          ] /command-not-found\n    3.1 MiB [          ] /ubuntu-advantage\n    1.5 MiB [          ] /grafana\n    1.1 MiB [          ] /fwupd\n  704.0 KiB [          ] /usbutils\n  612.0 KiB [          ] /systemd\n  208.0 KiB [          ] /cloud\n  124.0 KiB [          ] /ucf\n   40.0 KiB [          ] /polkit-1\n   36.0 KiB [          ] /PackageKit\n   28.0 KiB [          ] /pam\n   20.0 KiB [          ] /update-notifier\n   16.0 KiB [          ] /grub\n   12.0 KiB [          ] /update-manager\n</code></pre> <p>In this example, you can see that Geth is using 929.4 GiB, and Lighthouse is using 64.7 GiB since they were configured to store their databases under <code>/var/lib</code>.</p> <p>On linux, you can run <code>lsblk</code> to list your block devices in a tree-like format. It will return something like this:</p> <pre><code>NAME                      MAJ:MIN RM   SIZE RO TYPE MOUNTPOINTS\nloop0                       7:0    0  63.4M  1 loop /snap/core20/1974\nloop1                       7:1    0  63.9M  1 loop /snap/core20/2105\nloop2                       7:2    0 111.9M  1 loop /snap/lxd/24322\nloop3                       7:3    0 114.4M  1 loop /snap/lxd/26741\nloop4                       7:4    0  53.3M  1 loop /snap/snapd/19457\nloop5                       7:5    0  40.4M  1 loop /snap/snapd/20671\nnvme0n1                   259:0    0   1.8T  0 disk\n\u251c\u2500nvme0n1p1               259:1    0     1G  0 part /boot/efi\n\u251c\u2500nvme0n1p2               259:2    0     2G  0 part /boot\n\u2514\u2500nvme0n1p3               259:3    0   1.8T  0 part\n  \u2514\u2500ubuntu--vg-ubuntu--lv 253:0    0   1.8T  0 lvm  /\n</code></pre> <p>In this example, you can see the only disk being <code>nvme0n1</code> with 3 partitions and a logical volume using LVM.</p>"},{"location":"migrating-to-a-larger-disk/#pruning","title":"Pruning","text":"<p>Some clients like Geth and Nethermind support pruning your existing database to lower its disk usage. The general idea is that these clients accumulate data that can be removed with a manual or automatic process.</p> <p>With the latest stable release for Geth, the default options is to use the path state storage scheme (PBSS) and the pebble database. These options are the best in terms of disk usage and they do not require any pruning. If you performed your initial sync with an older version, you might still be using the old hash state scheme and you will accumulate unneeded data over time. You will need to stop Geth, perform a manual process to prune its database, and restart it to remove this junk. During this time, your node will be offline, and staking penalties will start accruing. The rescue node project can be a good solution for stakers to prevent or lower downtime and penalties. Pruning Geth also needs around 80 GB of free space to begin with on Mainnet. If you don't have that free space, you could try to delete your consensus client database to make enough room for it and resync your consensus client using a checkpoint sync endpoint. You should consider a resync from scratch with the default path state scheme (PBSS) if you are still on the old hash state scheme for long-term low disk usage. Make sure you are using the latest stable Geth version first. During a normal Geth start, Geth should output which storage scheme it is using during one of the first few log messages.</p> <p>With the latest stable release for Nethermind, you have different configurations that enable online pruning where your client is still able to serve its normal operation and clean its database at the same time. There is a manual process where you can trigger the pruning process. There is one based on a database size threshold. There is one based on a remaining storage space threshold. In any case, you are going to need around 230 GB of free space to begin that pruning process on Mainnet. A good strategy is to set the automatic pruning configuration to trigger when you are slightly above that 230 GB of remaining free space. This implies that you are effectively reserving that space exclusively for pruning.</p> <p>With Lighthouse, you can prune historic states if you synced your beacon node before version 4.4.1.</p>"},{"location":"migrating-to-a-larger-disk/#updating-and-resyncing-from-scratch","title":"Updating and Resyncing from Scratch","text":"<p>Clients implement improvements over time to their disk usage strategy. Updating your client and resyncing from scratch can not only remove the unneeded data that was accumulated over time, but it can also enable your client to use better ways of storing that data to use less disk space. For execution clients, you are likely going to experience some extended downtime (a few hours to a few days). The rescue node project can be a good solution for stakers to prevent or lower downtime and penalties. For consensus clients, you can often use a checkpoint sync endpoint to resync from scratch in a few minutes for minimal downtime.</p> <p>For example, Lighthouse beacon nodes that were synced before version 4.4.1 have historic states that are no longer needed. Performing the prune-states command or resyncing from scratch will drastically lower disk usage.</p>"},{"location":"migrating-to-a-larger-disk/#resyncing-with-a-different-configuration","title":"Resyncing with a Different Configuration","text":"<p>Clients often have a large number of configuration options that can influence disk usage.</p> <p>With Geth, starting with version 1.13.0, you can use a Path Based Storage Scheme (PBSS) to avoid storing unnecessary data in your database. This new storage scheme is enabled by default with the latest stable release version. It has a good impact on long-term disk usage and on avoiding the pruning maintenance task. You will need to resync from scratch if you want to use this configuration and your database or storage scheme is using the old one. During this time, your node will be offline, and staking penalties will start accruing. The rescue node project can be a good solution for stakers to prevent or lower downtime and penalties.</p> <p>With Nethermind, the development team is exploring 3 approaches to improve disk usage: Paprika, Path-Based Storage, and Half-path. As of today, November 2024, half-path is the default state storage for Nethermind. If your database is still using the old hash state storage, there are a migration path to the half-path state storage.</p>"},{"location":"migrating-to-a-larger-disk/#migrating-to-a-different-client","title":"Migrating to a Different Client","text":"<p>Different clients and configurations can lead to lower disk usage. Migrating from a client that does not have a great disk usage strategy to one that does can help with the disk usage concern. As of today, November 2024, a freshly synced Nethermind client with its default half-path state storage is one of the best execution clients and configurations in terms of disk usage.</p>"},{"location":"migrating-to-a-larger-disk/#using-multiple-disks","title":"Using Multiple Disks","text":"<p>If your machine supports installing multiple good SSDs and your system is configured to enable extending your partitions or volumes easily, such as with LVM, adding another disk can be an interesting alternative to migrating to a larger disk. This approach can limit the downtime it takes to gain more disk space. It has a con in terms of larger risks coming from a disk failure. Your new machine will now fail if either one of your disks fails instead of the risk coming from a single disk. Using various RAID setups can help alleviate the risks from disk failures in a multiple disks configuration, but it's probably overkill for home stakers and home node operators. RAID setups are out of scope for this guide.</p>"},{"location":"migrating-to-a-larger-disk/#monitoring-and-alerting","title":"Monitoring and Alerting","text":"<p>A good practice is to install and configure monitoring and alerting tools to watch your machine's free disk space.</p>"},{"location":"migrating-to-a-larger-disk/#migrating-to-a-larger-disk","title":"Migrating to a Larger Disk","text":"<p>The general idea to migrate to a larger disk is simple.</p> <ol> <li>Stop your staking or node machine.</li> <li>Plug both disks into the same machine (it can be the same staking or node machine, or it can be another one).</li> <li>Boot into a live OS or a tool to perform the copy.</li> <li>Extend your volumes, your partitions and your filesystems on the new larger disk to be able to use the full capacity.</li> <li>Plug the larger disk into your staking or node machine and restart that machine.</li> </ol> <p>From there, your machine should simply work as it did before with more disk space for its operations. Extending your volumes, partitions and filesystems (step 4) can be performed during the copying step, after the copying step, or even after restarting your staking or node machine.</p>"},{"location":"migrating-to-a-larger-disk/#requirements","title":"Requirements","text":"<p>You will need a machine where you can plug both disks at the same time. If you don't have enough room for both SSDs, you can buy an adapter like one you plug into a USB port. You will need a USB stick to put your live OS or tools on. Tools and general support to perform this on the Apple M1, M2 or M3 platform is limited. Asahi Linux and their derivative work might be useful instead of using Clonezilla. Various native Apple software could also work. We will focus on using an amd64 platform to perform the migration for this guide.</p> <p>There are commercial software alternatives to using Clonezilla (EaseUS Todo Backup, Macrium Reflect, Acronis Cyber Protect Home Office, AOMEI Backupper, etc) for disk cloning. The main pros of using Clonezilla are that it is free and open source. The cons of using Clonezilla include limited support and a text-based user interface. Rescuezilla is a nice GUI on top of Clonezilla if you are intimidated by the text-based user interface.</p>"},{"location":"migrating-to-a-larger-disk/#preparing-your-usb-stick","title":"Preparing your USB stick","text":"<p>This can be done on any machine connected to the Internet.</p> <ol> <li>Use an empty USB stick or a USB stick that does not contain any important document. Plug it in.</li> <li>Download the latest stable release of Clonezilla Live. The ISO version for amd64 CPU architecture is likely what you will want.</li> <li>Download Rufus (On Windows) or balena Etcher (On macOS or Linux) to create your bootable USB stick.</li> <li>Run Rufus or balena Etcher, select the Clonezilla Live archive you downloaded at step 2 and select your USB stick. Create your bootable USB stick by following the software instructions.</li> </ol>"},{"location":"migrating-to-a-larger-disk/#copying-the-content-on-the-larger-disk","title":"Copying the content on the larger disk","text":"<ol> <li>Stop the machine you want to use to perform the disk copy.</li> <li>Plug in both SSDs in the same machine.</li> <li>Plug in your bootable USB stick.</li> <li>Boot your machine using the USB stick image and OS. The actual instructions to boot onto a USB stick will vary from machine to machine.</li> </ol> <p>There are some common keyboard keys that are often used during the boot process to access the boot menu or BIOS/UEFI settings where you can choose the boot device. Here are a few possibilities:</p> <ul> <li>F2 or Del: On many systems, pressing the F2 key or the Del key during the initial startup process will take you to the BIOS or UEFI settings. From there, you can navigate to the Boot menu and select the USB drive as the boot device.</li> <li>F12 or Esc: Some systems use the F12 key or the Esc key to directly access the boot menu during startup. Pressing either of these keys should bring up a menu where you can choose the USB drive as the boot option.</li> <li>F10 or F9: On certain computers, the F10 or F9 key might be used to access the Boot menu directly during startup.</li> <li>ESC or Tab: In some cases, pressing the ESC key or the Tab key during startup may display a boot menu where you can select the USB drive.</li> </ul> <p>Keep in mind that the exact key and the process can vary, so it's recommended to check your computer's manual or look for on-screen prompts during startup. If you're still unsure, you can provide the make and model of your computer, we can try to find it for you.</p> <ol> <li>From the live OS or tool you installed on the USB stick, select to copy or clone from a local disk to another local disk. If you are using Clonezilla, there is also an option to extend the new partitions to use all the available space (<code>-k1</code> Create partition table proportionally). Enable that option.</li> </ol> <p>When booting from the Clonezilla Live you can choose the default option, Clonezilla live (VGA 800x600).</p> <p></p> <p>Start the regular text-based user interface by choosing the default option, Start Clonezilla.</p> <p></p> <p>Select the device-device mode to clone a full disk to another one.</p> <p></p> <p>Choose the Beginner mode for advanced parameters.</p> <p></p> <p>Select the disk_to_local_disk clone mode.</p> <p></p> <p>Select your small disk as the source disk.</p> <p></p> <p>Select your larger disk as the target disk.</p> <p></p> <p>You can select the -sfsck option to skip checking/repairing the source file system.</p> <p></p> <p>If you want to automatically expand your partitions, select the -k1 option to Create partition table proportionally on the larger disk. If you do not select this option, you will need to manually expand your partitions afterwards.</p> <p></p> <p>Finally, you can select an action to be performed when Clonezilla is done with its task.</p> <p></p> <p>Once you answer all these questions, you will be asked to confirm everything. As Clonezilla perform the disk clone, you will see a progress dialog displaying an estimate of the remaining time.</p> <p></p> <ol> <li>Wait for the cloning process to complete. This can take a few hours depending on the copying speed.</li> <li>Shut down the machine.</li> <li>Remove both SSDs.</li> </ol>"},{"location":"migrating-to-a-larger-disk/#restarting-your-machine","title":"Restarting your machine","text":"<p>Once all the data is copied onto that larger SSD, plug that larger disk into your staking or node machine. Restart that machine and you should be good to go.</p>"},{"location":"migrating-to-a-larger-disk/#resizing-your-volumes-with-lvm","title":"Resizing your volumes with LVM","text":"<p>LVM adds a few interesting features and it is the default option when installing and configuring a new Ubuntu 22.04 system. It also adds some complexity when migrating to a larger disk. If you are using LVM, you will need to resize your physical volume, you will need to extend your logical volume and you will likely need to resize your filesystem. This can be done after rebooting on your larger disk while it is running. This can be done with these commands:</p> <pre><code>sudo pvresize [physical volume path]\nsudo lvextend -l +100%FREE --resizefs [logical volume path]\n</code></pre> <p>where <code>[physical volume path]</code> is replaced with your physical volume path and <code>[logical volume path]</code> is replaced with your logical volume path.</p> <p>Here is a concrete example of using this command.</p> <pre><code>sudo pvresize /dev/nvme0n1p3\nsudo lvextend -l +100%FREE --resizefs /dev/ubuntu-vg/ubuntu-lv\n</code></pre> <p>You can list all your system physical volumes, volume groups and logical volumes including their paths with <code>sudo vgdisplay -v</code>. Including the <code>--resizefs</code> flag with the <code>lvextend</code> command above will automatically resize the underlying filesystem meaning that you can skip the next section.</p>"},{"location":"migrating-to-a-larger-disk/#resizing-your-filesystem","title":"Resizing your filesystem","text":"<p>Some filesystems will need to be resized in order to account for their underlying partition size change. If you are using ext4 which is common on Linux, you can simply execute <code>sudo resize2fs [filesystem device]</code> where <code>[filesystem device]</code> is replaced with your filesystem device path. You can list all the filesystems and their type with <code>df -T</code> to obtain your filesystem device path. This can be done after rebooting on your larger disk while it is running.</p> <p>Here is a concrete example of using this command.</p> <pre><code>sudo resize2fs /dev/ubuntu-vg/ubuntu-lv\n</code></pre>"},{"location":"migrating-to-a-larger-disk/#whats-next","title":"What's next?","text":"<p>Once you confirm that the larger SSD works fine, you should format and erase all the data from your smaller disk especially if you are running a staking machine. The main concern is running both of these disks at the same time and potentially creating a slashing event.</p>"},{"location":"migrating-to-a-larger-disk/#support","title":"Support","text":"<p>If you have any question or if you need additional support, make sure to get in touch with the ethstaker community on:</p> <ul> <li>Discord: dsc.gg/ethstaker</li> <li>Reddit: reddit.com/r/ethstaker</li> </ul>"},{"location":"monitoring/","title":"Guide on how to do monitoring for an Ethereum validator","text":"<p>Monitoring your system resources is an important task for any system administrator using any kind of machine whether you are a professional managing a large data center or simply someone tinkering at home.</p> <p>This guide is meant for people with no or little experience in monitoring. It will show you step by step how to do monitoring on your machine by giving you the instructions to install and configure all the tools needed. It will assume you are using a modern linux distribution with systemd and APT (like Ubuntu 20.04) on a modern x86 CPU (Intel, AMD).</p> <p>A video tutorial of this guide can be seen on https://youtu.be/Jkvyd8k_R9Y .</p>"},{"location":"monitoring/#why-would-you-want-to-do-monitoring","title":"Why would you want to do monitoring?","text":"<p>Here are some good reasons why you might want to do monitoring on your machine:</p> <ol> <li>Information visibility: You want to expose and be able to easily see your machine details.</li> <li>Issue tracking and debugging: You want to be able to inspect what happened in the past and see clearly how your machine reacted to some event.</li> <li>Issue prevention: You want to be able to see potential resources exhaustion ahead of time.</li> </ol>"},{"location":"monitoring/#overview","title":"Overview","text":"<p>We will install 3 tools with this guide: Prometheus, Node Exporter and Grafana.</p> <p>Prometheus is an open-source systems monitoring project. It collects and stores different metrics in a specialized database. It provides all those metrics to any other tool who wants to query them in a flexible, efficient and easy way. In our setup, it will collect metrics from Node Exporter and optionally from Ethereum clients and it will provide them on-demand to Grafana.</p> <p>Node Exporter is an open-source project that exposes your hardware and OS metrics. In our setup, it will provide your system metrics to Prometheus.</p> <p>Grafana is an open-source project used to visualize metrics. It can be used to create dashboards that easily show the metrics you are interested in. In our setup, it will query the metrics stored on Prometheus to show them in a browser with nice charts and diagrams.</p> <p></p>"},{"location":"monitoring/#executing-the-commands","title":"Executing the commands","text":"<p>Almost all of these commands will be performed in a terminal. Start your Terminal application. Any line that starts with the dollar sign (<code>$</code>) is a command that need to be executed in your terminal. Do not input the dollar sign (<code>$</code>) in your terminal, only the text that comes after that.</p> <p>Executing a command with <code>sudo</code> will occasionally ask you for your password. Make sure to enter your account password correctly. You can execute the command again if you fail to enter the correct password after a few attempts.</p>"},{"location":"monitoring/#installing-node-exporter","title":"Installing Node Exporter","text":"<p>Create a user account for the service to run under. This account will not be able to log into the machine. It will only be used to run the service.</p> <pre><code>$ sudo useradd --no-create-home --shell /bin/false node_exporter\n</code></pre> <p>Download the latest stable version of Node Exporter from https://prometheus.io/download/#node_exporter (avoid any pre-release version). As of this date, the latest stable release version is 1.8.2 . Adjust the following instructions accordingly if there is a newer stable release version with a different archive name. The file name should end with linux-amd64.tar.gz (for linux and AMD64 instructions set).</p> <pre><code>$ wget https://github.com/prometheus/node_exporter/releases/download/v1.8.2/node_exporter-1.8.2.linux-amd64.tar.gz\n</code></pre> <p>Verify that the SHA256 Checksum as shown on https://prometheus.io/download/#node_exporter is the same as the file we just downloaded.</p> <pre><code>$ sha256sum node_exporter-1.8.2.linux-amd64.tar.gz\n</code></pre> <p>Extract the archive.</p> <pre><code>$ tar xvf node_exporter-1.8.2.linux-amd64.tar.gz\n</code></pre> <p>Copy the binary to the following location and set ownership.</p> <pre><code>$ sudo cp node_exporter-1.8.2.linux-amd64/node_exporter /usr/local/bin\n$ sudo chown -R node_exporter:node_exporter /usr/local/bin/node_exporter\n</code></pre> <p>Remove the download leftovers.</p> <pre><code>$ rm -rf node_exporter-1.8.2.linux-amd64\n$ rm node_exporter-1.8.2.linux-amd64.tar.gz\n</code></pre> <p>Create a systemd service file to store the service config which tells systemd to run Node Exporter as the node_exporter user.</p> <pre><code>$ sudo nano /etc/systemd/system/node_exporter.service\n</code></pre> <p>Paste the following into the file. Exit and save.</p> <pre><code>[Unit]\nDescription=Node Exporter\nWants=network-online.target\nAfter=network-online.target\n\n[Service]\nType=simple\nUser=node_exporter\nGroup=node_exporter\nRestart=always\nRestartSec=5\nExecStart=/usr/local/bin/node_exporter \\\n    --collector.systemd \\\n    --collector.processes \\\n    --web.listen-address=\"localhost:9100\"\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <p>Reload systemd to reflect the changes.</p> <pre><code>$ sudo systemctl daemon-reload\n</code></pre> <p>And then start the service with the following command and check the status to make sure it\u2019s running correctly.</p> <pre><code>$ sudo systemctl start node_exporter\n$ sudo systemctl status node_exporter\n</code></pre> <p>Output should look something like this.</p> <pre><code>\u25cf node_exporter.service - Node Exporter\n     Loaded: loaded (/etc/systemd/system/node_exporter.service; disabled; vendo&gt;\n     Active: active (running) since Wed 2021-08-04 10:48:25 EDT; 4s ago\n   Main PID: 10984 (node_exporter)\n      Tasks: 5 (limit: 18440)\n     Memory: 2.4M\n     CGroup: /system.slice/node_exporter.service\n             \u2514\u250010984 /usr/local/bin/node_exporter\n\nAug 04 10:48:25 remy-MINIPC-PN50 node_exporter[10984]: level=info ts=2021-08-04&gt;\nAug 04 10:48:25 remy-MINIPC-PN50 node_exporter[10984]: level=info ts=2021-08-04&gt;\nAug 04 10:48:25 remy-MINIPC-PN50 node_exporter[10984]: level=info ts=2021-08-04&gt;\nAug 04 10:48:25 remy-MINIPC-PN50 node_exporter[10984]: level=info ts=2021-08-04&gt;\nAug 04 10:48:25 remy-MINIPC-PN50 node_exporter[10984]: level=info ts=2021-08-04&gt;\nAug 04 10:48:25 remy-MINIPC-PN50 node_exporter[10984]: level=info ts=2021-08-04&gt;\nAug 04 10:48:25 remy-MINIPC-PN50 node_exporter[10984]: level=info ts=2021-08-04&gt;\nAug 04 10:48:25 remy-MINIPC-PN50 node_exporter[10984]: level=info ts=2021-08-04&gt;\nAug 04 10:48:25 remy-MINIPC-PN50 node_exporter[10984]: level=info ts=2021-08-04&gt;\nAug 04 10:48:25 remy-MINIPC-PN50 node_exporter[10984]: level=info ts=2021-08-04&gt;\n</code></pre> <p>If you did everything right, it should say active (running) in green. If not then go back and repeat the steps to fix the problem. Press Q to quit. Finally, enable Node Exporter to start on boot.</p> <pre><code>$ sudo systemctl enable node_exporter\n</code></pre>"},{"location":"monitoring/#installing-prometheus","title":"Installing Prometheus","text":"<p>Create a user account for the service to run under. This account will not be able to log into the machine. It will only be used to run the service.</p> <pre><code>$ sudo useradd --no-create-home --shell /bin/false prometheus\n</code></pre> <p>Create the configuration and data directories with proper ownership.</p> <pre><code>$ sudo mkdir /etc/prometheus\n$ sudo mkdir /var/lib/prometheus\n$ sudo chown -R prometheus:prometheus /etc/prometheus\n$ sudo chown -R prometheus:prometheus /var/lib/prometheus\n</code></pre> <p>Download the latest stable version of Prometheus from https://prometheus.io/download/#prometheus (avoid any pre-release version). As of this date, the latest stable release version is 2.54.1 . Adjust the following instructions accordingly if there is a newer stable release version with a different archive name. The file name should end with linux-amd64.tar.gz (for linux and AMD64 instructions set).</p> <pre><code>$ wget https://github.com/prometheus/prometheus/releases/download/v3.0.0/prometheus-3.0.0.linux-amd64.tar.gz\n</code></pre> <p>Verify that the SHA256 Checksum as shown on https://prometheus.io/download/#prometheus is the same as the file we just downloaded.</p> <pre><code>$ sha256sum prometheus-3.0.0.linux-amd64.tar.gz\n</code></pre> <p>Extract the archive.</p> <pre><code>$ tar xvf prometheus-3.0.0.linux-amd64.tar.gz\n</code></pre> <p>Copy the binaries to the following locations and set ownership.</p> <pre><code>$ sudo cp prometheus-3.0.0.linux-amd64/prometheus /usr/local/bin/\n$ sudo cp prometheus-3.0.0.linux-amd64/promtool /usr/local/bin/\n$ sudo chown -R prometheus:prometheus /usr/local/bin/prometheus\n$ sudo chown -R prometheus:prometheus /usr/local/bin/promtool\n</code></pre> <p>Remove the download leftovers.</p> <pre><code>$ rm -rf prometheus-3.0.0.linux-amd64\n$ rm prometheus-3.0.0.linux-amd64.tar.gz\n</code></pre> <p>Setup the Prometheus configuration file. Open the YAML config file for editing.</p> <pre><code>$ sudo nano /etc/prometheus/prometheus.yml\n</code></pre> <p>Paste the following into the file taking care not to make any additional edits. Exit and save the file.</p> <pre><code>global:\n  scrape_interval:     15s # Set the scrape interval to every 15 seconds. Default is every 1 minute.\n  evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute.\n\n# Alertmanager configuration\nalerting:\n  alertmanagers:\n  - static_configs:\n    - targets:\n      # - alertmanager:9093\nrule_files:\n  # - \"first_rules.yml\"\n  # - \"second_rules.yml\"\n\nscrape_configs:\n  - job_name: 'node_exporter'\n    static_configs:\n      - targets: ['localhost:9100']\n</code></pre> <p>The <code>scrape_configs</code> section define the different jobs where Prometheus will poll data from. We have 1 job so far in this configuration file: node_exporter. It will poll data from Node Exporter and it will store all your hardware and OS metrics in its database.</p> <p>Set ownership for the config file. The prometheus account will own this.</p> <pre><code>$ sudo chown -R prometheus:prometheus /etc/prometheus/prometheus.yml\n</code></pre> <p>Finally, let\u2019s test the service is running correctly.</p> <pre><code>$ sudo -u prometheus /usr/local/bin/prometheus \\\n    --config.file /etc/prometheus/prometheus.yml \\\n    --storage.tsdb.path /var/lib/prometheus/ \\\n    --web.listen-address=\"localhost:9090\"\n</code></pre> <p>Output should look something like this. Press <code>Ctrl</code> + <code>C</code> to exit.</p> <pre><code>level=info ts=2021-08-04T16:18:42.042Z caller=main.go:981 msg=\"Loading configuration file\" filename=/etc/prometheus/prometheus.yml\nlevel=info ts=2021-08-04T16:18:42.042Z caller=main.go:1012 msg=\"Completed loading of configuration file\" filename=/etc/prometheus/prometheus.yml totalDuration=708.063\u00b5s remote_storage=3.096\u00b5s web_handler=719ns query_engine=1.658\u00b5s scrape=94.392\u00b5s scrape_sd=59.705\u00b5s notify=41.158\u00b5s notify_sd=22.063\u00b5s rules=2.437\u00b5s\nlevel=info ts=2021-08-04T16:18:42.043Z caller=main.go:796 msg=\"Server is ready to receive web requests.\"\n</code></pre> <p>Create a systemd service file to store the service config which tells systemd to run Prometheus as the prometheus user, with the configuration file located in the <code>/etc/prometheus/prometheus.yml</code> directory, and to store its data in the <code>/var/lib/prometheus directory</code>.</p> <pre><code>$ sudo nano /etc/systemd/system/prometheus.service\n</code></pre> <p>Paste the following into the file. Exit and save.</p> <pre><code>[Unit]\nDescription=Prometheus\nWants=network-online.target\nAfter=network-online.target\n\n[Service]\nType=simple\nUser=prometheus\nGroup=prometheus\nRestart=always\nRestartSec=5\nExecStart=/usr/local/bin/prometheus \\\n    --config.file /etc/prometheus/prometheus.yml \\\n    --storage.tsdb.path /var/lib/prometheus/ \\\n    --storage.tsdb.retention.time=15d \\\n    --web.listen-address=\"localhost:9090\"\nExecReload=/bin/kill -HUP $MAINPID\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <p>A configuration option to consider here is the <code>--storage.tsdb.retention.time</code> flag which tells Prometheus how long to retain samples (data or metrics) in storage. The default value is 15 days (<code>15d</code>) meaning that Prometheus will start purging your metrics once they are 15 days old or older. This is a great way to limit Prometheus storage usage as a compromise to having a shorter view of what happened in the past. Older metrics generally have a lower value. You can tweak that value in your service configuration file above to your liking (Units Supported: y, w, d, h, m, s, ms). Longer retention periods will consume more storage. You should probably provision at least 1 GB per 15 days of retention for a single machine.</p> <p>Reload systemd to reflect the changes.</p> <pre><code>$ sudo systemctl daemon-reload\n</code></pre> <p>And then start the service with the following command and check the status to make sure it\u2019s running correctly.</p> <pre><code>$ sudo systemctl start prometheus\n$ sudo systemctl status prometheus\n</code></pre> <p>Output should look something like this.</p> <pre><code>\u25cf prometheus.service - Prometheus\n     Loaded: loaded (/etc/systemd/system/prometheus.service; disabled; vendor p&gt;\n     Active: active (running) since Wed 2021-08-04 12:20:56 EDT; 4s ago\n   Main PID: 12061 (prometheus)\n      Tasks: 11 (limit: 18440)\n     Memory: 19.6M\n     CGroup: /system.slice/prometheus.service\n             \u2514\u250012061 /usr/local/bin/prometheus --config.file /etc/prometheus/pr&gt;\n\nAug 04 12:20:56 remy-MINIPC-PN50 prometheus[12061]: level=info ts=2021-08-04T16&gt;\nAug 04 12:20:56 remy-MINIPC-PN50 prometheus[12061]: level=info ts=2021-08-04T16&gt;\nAug 04 12:20:56 remy-MINIPC-PN50 prometheus[12061]: level=info ts=2021-08-04T16&gt;\nAug 04 12:20:56 remy-MINIPC-PN50 prometheus[12061]: level=info ts=2021-08-04T16&gt;\nAug 04 12:20:56 remy-MINIPC-PN50 prometheus[12061]: level=info ts=2021-08-04T16&gt;\nAug 04 12:20:56 remy-MINIPC-PN50 prometheus[12061]: level=info ts=2021-08-04T16&gt;\nAug 04 12:20:56 remy-MINIPC-PN50 prometheus[12061]: level=info ts=2021-08-04T16&gt;\nAug 04 12:20:56 remy-MINIPC-PN50 prometheus[12061]: level=info ts=2021-08-04T16&gt;\nAug 04 12:20:56 remy-MINIPC-PN50 prometheus[12061]: level=info ts=2021-08-04T16&gt;\nAug 04 12:20:56 remy-MINIPC-PN50 prometheus[12061]: level=info ts=2021-08-04T16&gt;\n</code></pre> <p>If you did everything right, it should say active (running) in green. If not then go back and repeat the steps to fix the problem. Press Q to quit. Lastly, enable Prometheus to start on boot.</p> <pre><code>$ sudo systemctl enable prometheus\n</code></pre>"},{"location":"monitoring/#installing-grafana","title":"Installing Grafana","text":"<p>We will use the Install from APT repository installation guide from Grafana. A benefit of using this method is that it creates the service for you and it uses APT to easily keep Grafana up-to-date.</p> <p>Install the prerequisites.</p> <pre><code>$ sudo apt update -y\n$ sudo apt install -y apt-transport-https software-properties-common wget\n</code></pre> <p>Import the Grafana PGP key.</p> <pre><code>$ sudo wget -q -O /usr/share/keyrings/grafana.key https://apt.grafana.com/gpg.key\n</code></pre> <p>Add the Grafana APT repository for stable releases.</p> <pre><code>$ echo \"deb [signed-by=/usr/share/keyrings/grafana.key] https://apt.grafana.com stable main\" | sudo tee -a /etc/apt/sources.list.d/grafana.list\n</code></pre> <p>Install Grafana from the APT repository.</p> <pre><code>$ sudo apt update\n$ sudo apt install grafana\n</code></pre> <p>Open the Grafana configuration file.</p> <pre><code>$ sudo nano /etc/grafana/grafana.ini\n</code></pre> <p>Search for the following section.</p> <pre><code>[server]\n# Protocol (http, https, h2, socket)\n;protocol = http\n\n# The ip address to bind to, empty will bind to all interfaces\n;http_addr =\n</code></pre> <p>Modify the configuration file by removing the semicolon (<code>;</code>) in front of <code>http_addr</code> and by adding <code>localhost</code> after the equal sign (<code>=</code>). It should look like this. Exit and save.</p> <pre><code>[server]\n# Protocol (http, https, h2, socket)\n;protocol = http\n\n# The ip address to bind to, empty will bind to all interfaces\nhttp_addr = 127.0.0.1\n</code></pre> <p>Reload systemd to reflect the changes.</p> <pre><code>$ sudo systemctl daemon-reload\n</code></pre> <p>And then start the service with the following command and check the status to make sure it\u2019s running correctly.</p> <pre><code>$ sudo systemctl start grafana-server\n$ sudo systemctl status grafana-server\n</code></pre> <p>Output should look something like this.</p> <pre><code>\u25cf grafana-server.service - Grafana instance\n     Loaded: loaded (/lib/systemd/system/grafana-server.service; disabled; vend&gt;\n     Active: active (running) since Wed 2021-08-04 12:59:12 EDT; 4s ago\n       Docs: http://docs.grafana.org\n   Main PID: 13876 (grafana-server)\n      Tasks: 12 (limit: 18440)\n     Memory: 31.2M\n     CGroup: /system.slice/grafana-server.service\n             \u2514\u250013876 /usr/sbin/grafana-server --config=/etc/grafana/grafana.ini&gt;\n\nAug 04 12:59:13 remy-MINIPC-PN50 grafana-server[13876]: t=2021-08-04T12:59:13-0&gt;\nAug 04 12:59:13 remy-MINIPC-PN50 grafana-server[13876]: t=2021-08-04T12:59:13-0&gt;\nAug 04 12:59:13 remy-MINIPC-PN50 grafana-server[13876]: t=2021-08-04T12:59:13-0&gt;\nAug 04 12:59:13 remy-MINIPC-PN50 grafana-server[13876]: t=2021-08-04T12:59:13-0&gt;\nAug 04 12:59:13 remy-MINIPC-PN50 grafana-server[13876]: t=2021-08-04T12:59:13-0&gt;\nAug 04 12:59:13 remy-MINIPC-PN50 grafana-server[13876]: t=2021-08-04T12:59:13-0&gt;\nAug 04 12:59:13 remy-MINIPC-PN50 grafana-server[13876]: t=2021-08-04T12:59:13-0&gt;\nAug 04 12:59:13 remy-MINIPC-PN50 grafana-server[13876]: t=2021-08-04T12:59:13-0&gt;\nAug 04 12:59:13 remy-MINIPC-PN50 grafana-server[13876]: t=2021-08-04T12:59:13-0&gt;\nAug 04 12:59:13 remy-MINIPC-PN50 grafana-server[13876]: t=2021-08-04T12:59:13-0&gt;\n</code></pre> <p>If you did everything right, it should say active (running) in green. If not then go back and repeat the steps to fix the problem. Press Q to quit. Lastly, enable Grafana to start on boot.</p> <pre><code>$ sudo systemctl enable grafana-server\n</code></pre>"},{"location":"monitoring/#accessing-grafana-and-adding-your-dashboards","title":"Accessing Grafana and adding your dashboards","text":"<p>To connect to Grafana, simply launch a browser from your machine and go to http://localhost:3000 . If you are remotely connecting to your machine with SSH or something else, there are a few security and privacy considerations for which you will need something like an SSH tunnel. Check out the Remote access to Grafana section if you are in this situation.</p> <p>The default credentials for Grafana are:</p> <ul> <li>Username: admin</li> <li>Password: admin</li> </ul> <p>After your first log in, you will be asked to change the password for the admin account. Make sure to enter a strong and unique password that you will be able to remember. That should bring you to your Grafana home page.</p> <p></p>"},{"location":"monitoring/#adding-the-prometheus-data-source","title":"Adding the Prometheus data source","text":"<p>Grafana has the ability to connect to multiple data sources. We have to add our Prometheus data source before we can use it. In the top left part of the page, click on the hamburger button (\u2630). Navigate in Connections &gt; Data sources. Click on the Add data source button. Select the Prometheus data source type. In the URL field, enter <code>http://localhost:9090</code>. Click the Save and test button at the bottom of the page. If everything is working, you should see a Successfully queried the Prometheus API message.</p> <p></p>"},{"location":"monitoring/#node-exporter-dashboard","title":"Node Exporter dashboard","text":"<p>A default installation does not include any dashboard. Let's add one for the hardware and OS metrics we are getting from Node Exporter. In the top left part of the page, click on the hamburger button (\u2630). Navigate in Dashboards. Click on the New dropdown button and click on Import. In the Import via grafana.com field, type <code>1860</code> and click on the Load button. On this next screen, make sure to select the Prometheus datasource from the dropdown list named Prometheus. Click on the Import button at the bottom.</p> <p></p> <p>This will lead you to a nice dashboard showing you a lot of information regarding your machine including CPU, Memory, Network and Disk usage. You can browse the different sections of this dashboard to see more information.</p> <p></p> <p>When viewing a dashboard, you will notice a few things:</p> <ul> <li>You can change everything in your dashboards. Panels can be moved, added, removed or changed to show what you prefer or to show it differently.</li> <li>Information is shown for a specific period. In the Node Export dashboard, you will notice a Last 24 hours period is selected in a dropdown list in the top right corner. That is your current viewing period which can be changed to your preferences.</li> <li>Information can automatically be refreshed. In the Node Export dashboard, you will notice a double circular arrows icon (\ud83d\udd04) with a dropdown button (\u02c5) in the top right part of the page. Using that dropdown button, you can select an automatic refresh delay like 1m for 1 minute.</li> </ul> <p>You can customize your experience in a lot of different ways. Check out the Grafana documentation to learn more about this.</p>"},{"location":"monitoring/#adding-monitoring-for-your-ethereum-clients","title":"Adding monitoring for your Ethereum clients","text":"<p>Adding monitoring for your other processes like your Ethereum clients usually requires 3 things:</p> <ol> <li>Enabling metrics collection and reporting on your client. This is usually done by adding a few flags to the command line arguments used to start your client and restarting your client service.</li> <li>Configuring Prometheus to poll the metrics from your client. This is done by adding a job in the <code>scrape_configs</code> section of the Prometheus configuration file and reloading your Prometheus service.</li> <li>Importing a new dashboard in Grafana to display those metrics. There are many public Grafana dashboards for a lot of different metrics, but sometimes you have to search hard to find one you like.</li> </ol> <p>The following sections will give you the details for each client. Depending on how you installed and configured those clients initially, the exact steps may vary.</p>"},{"location":"monitoring/#geth","title":"Geth","text":"<ol> <li>Add or make sure the following flags are included in the command line arguments used to start Geth: <code>--metrics --metrics.expensive --pprof</code>. Restart Geth. Make sure Geth is still running properly.</li> <li>Configure Prometheus to poll the metrics from Geth.</li> </ol> <p>Open your Prometheus configuration file.</p> <pre><code>$ sudo nano /etc/prometheus/prometheus.yml\n</code></pre> <p>Add the following job in your <code>scrape_configs</code> section below all the other jobs. Exit and save.</p> <pre><code>  - job_name: geth\n    scrape_timeout: 10s\n    metrics_path: /debug/metrics/prometheus\n    static_configs:\n      - targets: ['localhost:6060']\n</code></pre> <p>Reload Prometheus with this new configuration file.</p> <pre><code>$ sudo systemctl reload prometheus.service\n</code></pre> <p>Check your Prometheus logs to make sure the new configuration file was loaded correctly.</p> <pre><code>$ sudo journalctl -u prometheus.service -n 6\n</code></pre> <p>Output should look something like this. Press <code>q</code> to exit. Finding the Completed loading of configuration file message means your new configuration file was loaded correctly.</p> <pre><code>Aug 09 12:34:05 remy-MINIPC-PN50 systemd[1]: Reloading Prometheus.\nAug 09 12:34:05 remy-MINIPC-PN50 systemd[1]: Reloaded Prometheus.\nAug 09 12:34:05 remy-MINIPC-PN50 prometheus[1934]: level=info ts=2021-08-09T16:34:05.304Z caller=main.go:981 msg=\"Loading configuration file\" filename=/etc/prometheus/prometheus.yml\nAug 09 12:34:05 remy-MINIPC-PN50 prometheus[1934]: level=info ts=2021-08-09T16:34:05.311Z caller=main.go:1012 msg=\"Completed loading of configuration file\" filename=/etc/prometheus/prometheus.yml totalDuration=7.822144ms remote_storage=4.305\u00b5s web_handler=6.249\u00b5s query_engine=1.734\u00b5s scrape=1.831998ms scrape_sd=83.865\u00b5s notify=618.619\u00b5s notify_sd=51.754\u00b5s rules=1.993\u00b5s\n</code></pre> <ol> <li>Import a good Geth dashboard for Prometheus in Grafana.</li> </ol>"},{"location":"monitoring/#prysm","title":"Prysm","text":"<ol> <li>Metrics collection and reporting is enabled by default for Prysm. Nothing to do at this step.</li> <li>Open your Prometheus configuration file.</li> </ol> <pre><code>$ sudo nano /etc/prometheus/prometheus.yml\n</code></pre> <p>Add the following jobs in your <code>scrape_configs</code> section below all the other jobs. Exit and save.</p> <pre><code>  - job_name: prysm_validator\n    static_configs:\n      - targets: ['localhost:8081']\n  - job_name: prysm_beacon\n    static_configs:\n      - targets: ['localhost:8080']\n</code></pre> <p>Reload Prometheus with this new configuration file.</p> <pre><code>$ sudo systemctl reload prometheus.service\n</code></pre> <p>Check your Prometheus logs to make sure the new configuration file was loaded correctly.</p> <pre><code>$ sudo journalctl -u prometheus.service -n 6\n</code></pre> <p>Output should look something like this. Press <code>q</code> to exit. Finding the Completed loading of configuration file message means your new configuration file was loaded correctly.</p> <pre><code>Aug 09 12:34:05 remy-MINIPC-PN50 systemd[1]: Reloading Prometheus.\nAug 09 12:34:05 remy-MINIPC-PN50 systemd[1]: Reloaded Prometheus.\nAug 09 12:34:05 remy-MINIPC-PN50 prometheus[1934]: level=info ts=2021-08-09T16:34:05.304Z caller=main.go:981 msg=\"Loading configuration file\" filename=/etc/prometheus/prometheus.yml\nAug 09 12:34:05 remy-MINIPC-PN50 prometheus[1934]: level=info ts=2021-08-09T16:34:05.311Z caller=main.go:1012 msg=\"Completed loading of configuration file\" filename=/etc/prometheus/prometheus.yml totalDuration=7.822144ms remote_storage=4.305\u00b5s web_handler=6.249\u00b5s query_engine=1.734\u00b5s scrape=1.831998ms scrape_sd=83.865\u00b5s notify=618.619\u00b5s notify_sd=51.754\u00b5s rules=1.993\u00b5s\n</code></pre> <ol> <li>Import the Prysm dashboard for Prometheus in Grafana.</li> </ol>"},{"location":"monitoring/#lighthouse","title":"Lighthouse","text":"<ol> <li>Add or make sure the following flags are included in the command line arguments used to start the Lighthouse Beacon Node: <code>--validator-monitor-auto --metrics</code>. Restart the Lighthouse Beacon Node. Make sure the Lighthouse Beacon Node is still running properly.</li> <li>Configure Prometheus to poll the metrics from the Lighthouse Beacon Node.</li> </ol> <p>Open your Prometheus configuration file.</p> <pre><code>$ sudo nano /etc/prometheus/prometheus.yml\n</code></pre> <p>Add the following job in your <code>scrape_configs</code> section below all the other jobs. Exit and save.</p> <pre><code>  - job_name: lighthouse_beacon\n    static_configs:\n      - targets: ['localhost:5054']\n</code></pre> <p>Reload Prometheus with this new configuration file.</p> <pre><code>$ sudo systemctl reload prometheus.service\n</code></pre> <p>Check your Prometheus logs to make sure the new configuration file was loaded correctly.</p> <pre><code>$ sudo journalctl -u prometheus.service -n 6\n</code></pre> <p>Output should look something like this. Press <code>q</code> to exit. Finding the Completed loading of configuration file message means your new configuration file was loaded correctly.</p> <pre><code>Aug 09 12:34:05 remy-MINIPC-PN50 systemd[1]: Reloading Prometheus.\nAug 09 12:34:05 remy-MINIPC-PN50 systemd[1]: Reloaded Prometheus.\nAug 09 12:34:05 remy-MINIPC-PN50 prometheus[1934]: level=info ts=2021-08-09T16:34:05.304Z caller=main.go:981 msg=\"Loading configuration file\" filename=/etc/prometheus/prometheus.yml\nAug 09 12:34:05 remy-MINIPC-PN50 prometheus[1934]: level=info ts=2021-08-09T16:34:05.311Z caller=main.go:1012 msg=\"Completed loading of configuration file\" filename=/etc/prometheus/prometheus.yml totalDuration=7.822144ms remote_storage=4.305\u00b5s web_handler=6.249\u00b5s query_engine=1.734\u00b5s scrape=1.831998ms scrape_sd=83.865\u00b5s notify=618.619\u00b5s notify_sd=51.754\u00b5s rules=1.993\u00b5s\n</code></pre> <ol> <li>Import the Lighthouse Summary dashboard for Prometheus and the Lighthouse Validator Monitor dashboard for Prometheus in Grafana.</li> </ol> <p>As an alternative to those official dashboards from the Sigma Prime team, you can try a port of the Prysm Validator Dashboard for Lighthouse from the great Sea Monkey. This dashboard requires a few more instructions and it is assuming more technical knowledge from its users.</p>"},{"location":"monitoring/#teku","title":"Teku","text":"<ol> <li>Add or make sure the following flags are included in the command line arguments used to start Teku: <code>--metrics-enabled</code>. Restart Teku. Make sure Teku is still running properly.</li> <li>Configure Prometheus to poll the metrics from Teku.</li> </ol> <p>Open your Prometheus configuration file.</p> <pre><code>$ sudo nano /etc/prometheus/prometheus.yml\n</code></pre> <p>Add the following job in your <code>scrape_configs</code> section below all the other jobs. Exit and save.</p> <pre><code>  - job_name: teku\n    scrape_timeout: 10s\n    static_configs:\n      - targets: ['localhost:8008']\n</code></pre> <p>Reload Prometheus with this new configuration file.</p> <pre><code>$ sudo systemctl reload prometheus.service\n</code></pre> <p>Check your Prometheus logs to make sure the new configuration file was loaded correctly.</p> <pre><code>$ sudo journalctl -u prometheus.service -n 6\n</code></pre> <p>Output should look something like this. Press <code>q</code> to exit. Finding the Completed loading of configuration file message means your new configuration file was loaded correctly.</p> <pre><code>Aug 09 12:34:05 remy-MINIPC-PN50 systemd[1]: Reloading Prometheus.\nAug 09 12:34:05 remy-MINIPC-PN50 systemd[1]: Reloaded Prometheus.\nAug 09 12:34:05 remy-MINIPC-PN50 prometheus[1934]: level=info ts=2021-08-09T16:34:05.304Z caller=main.go:981 msg=\"Loading configuration file\" filename=/etc/prometheus/prometheus.yml\nAug 09 12:34:05 remy-MINIPC-PN50 prometheus[1934]: level=info ts=2021-08-09T16:34:05.311Z caller=main.go:1012 msg=\"Completed loading of configuration file\" filename=/etc/prometheus/prometheus.yml totalDuration=7.822144ms remote_storage=4.305\u00b5s web_handler=6.249\u00b5s query_engine=1.734\u00b5s scrape=1.831998ms scrape_sd=83.865\u00b5s notify=618.619\u00b5s notify_sd=51.754\u00b5s rules=1.993\u00b5s\n</code></pre> <ol> <li>Import the Teku dashboard for Prometheus in Grafana.</li> </ol>"},{"location":"monitoring/#nimbus","title":"Nimbus","text":"<ol> <li>Add or make sure the following flags are included in the command line arguments used to start the Nimbus Beacon Node: <code>--metrics</code>. Restart the Nimbus Beacon Node. Make sure the Nimbus Beacon Node is still running properly.</li> <li>Configure Prometheus to poll the metrics from the Nimbus Beacon Node.</li> </ol> <p>Open your Prometheus configuration file.</p> <pre><code>$ sudo nano /etc/prometheus/prometheus.yml\n</code></pre> <p>Add the following job in your <code>scrape_configs</code> section below all the other jobs. Exit and save.</p> <pre><code>  - job_name: nimbus\n    static_configs:\n      - targets: ['localhost:8008']\n</code></pre> <p>Reload Prometheus with this new configuration file.</p> <pre><code>$ sudo systemctl reload prometheus.service\n</code></pre> <p>Check your Prometheus logs to make sure the new configuration file was loaded correctly.</p> <pre><code>$ sudo journalctl -u prometheus.service -n 6\n</code></pre> <p>Output should look something like this. Press <code>q</code> to exit. Finding the Completed loading of configuration file message means your new configuration file was loaded correctly.</p> <pre><code>Aug 09 12:34:05 remy-MINIPC-PN50 systemd[1]: Reloading Prometheus.\nAug 09 12:34:05 remy-MINIPC-PN50 systemd[1]: Reloaded Prometheus.\nAug 09 12:34:05 remy-MINIPC-PN50 prometheus[1934]: level=info ts=2021-08-09T16:34:05.304Z caller=main.go:981 msg=\"Loading configuration file\" filename=/etc/prometheus/prometheus.yml\nAug 09 12:34:05 remy-MINIPC-PN50 prometheus[1934]: level=info ts=2021-08-09T16:34:05.311Z caller=main.go:1012 msg=\"Completed loading of configuration file\" filename=/etc/prometheus/prometheus.yml totalDuration=7.822144ms remote_storage=4.305\u00b5s web_handler=6.249\u00b5s query_engine=1.734\u00b5s scrape=1.831998ms scrape_sd=83.865\u00b5s notify=618.619\u00b5s notify_sd=51.754\u00b5s rules=1.993\u00b5s\n</code></pre> <ol> <li>Import the Nimbus dashboard for Prometheus in Grafana.</li> </ol>"},{"location":"monitoring/#security-risks","title":"Security risks","text":"<p>Adding Prometheus, Node Exporter and Grafana with this configuration comes with a few additional security risks for your machine.</p> <p>The first risk comes from the tools themselves. There might be some security issues with them that I am not aware of which might compromise your machine to some malicious actors. A great way to prevent such risk is to keep your system updated. Keeping Grafana updated should be somewhat easy as it was installed with APT. Executing those commands should keep all your system packages updated including Grafana.</p> <pre><code>$ sudo apt update\n$ sudo apt upgrade\n</code></pre> <p>Keeping Prometheus and Node Exporter updated will require more efforts. You will need to monitor new stable releases and whether they include severe or critical bug fixes. To update to a new version, you will need to download the latest stable release, extract the archive, copy the binaries to their expected location and restart these services. The process and the instructions to download the new version, extract the archive and copy the binaries is exactly the same one mentioned at the beginning of the Installing Node Exporter section and at the beginning of the Installing Prometheus section. Easy to use scripts that do all those steps for you are included below.</p> <p>To restart the Node Exporter service after you updated its binary, use this command.</p> <pre><code>$ sudo systemctl restart node_exporter\n</code></pre> <p>To restart the Prometheus service after you updated its binary, use this command.</p> <pre><code>$ sudo systemctl restart prometheus\n</code></pre> <p>You can find all the Prometheus releases and their changelog on https://github.com/prometheus/prometheus/releases . You can find all the Node Exporter releases and their changelog on https://github.com/prometheus/node_exporter/releases .</p> <p>You might never need to update these tools as there might not be any severe or critical issue with them or there might not be an issue that can be easily exploited by a malicious actor with the version you have. However, it's a good practice to monitor releases for the tools you are using and update them regularly.</p> <p>The second risk comes from the additional attack surface that these tools are creating. One of this attack surface is the HTTP servers they are adding and the ports on which they are listening. This guide configured them to only listen on your localhost interface meaning that they cannot be accessed from any other machine on a network. You would have to have malicious processes or actors connecting to these tools from your machine to access your private data for instance. A good way to prevent this risk is to limit the running processes, run only trusted processes, limit who can connect to the machine and only allow trusted people connecting to your machine.</p> <p>There might be other kind of risks associated with those tools and this configuration, but I think these two are the main ones.</p>"},{"location":"monitoring/#prometheus-and-node-exporter-update-scripts","title":"Prometheus and Node Exporter update scripts","text":"<p>I created some simple scripts to ease the pain of updating Prometheus and Node Exporter. You can find them in this repository: update-prometheus.py and update-node-exporter.py. Those scripts will check the current installed version and they will compare it with the latest stable release version on Github. If there is a new version available, it will prompt you to update it. Those scripts require a somewhat recent version of Python 3 which should already be installed on your system.</p> <p>You can easily download those scripts with:</p> <pre><code>$ wget https://raw.githubusercontent.com/eth-educators/ethstaker-guides/main/docs/scripts/update-prometheus.py\n$ wget https://raw.githubusercontent.com/eth-educators/ethstaker-guides/main/docs/scripts/update-node-exporter.py\n</code></pre> <p>To run the Prometheus update script, use this command:</p> <pre><code>$ python3 update-prometheus.py\n</code></pre> <p>To run the Node Exporter update script, use this command:</p> <pre><code>$ python3 update-node-exporter.py\n</code></pre>"},{"location":"monitoring/#remote-access-to-grafana","title":"Remote access to Grafana","text":"<p>If you want to access Grafana and your dashboards from a remote machine on your local network or from the internet, you could simply use an SSH tunnel (with PuTTY on Windows or natively on Windows 10). There are various other alternatives for remote access that might better suits your needs. A secure HTTPS reverse proxy or a local VPN are possible solutions to this problem but they will be left as an exercise to the reader.</p> <p>If you are using the main <code>ssh</code> CLI client, you can often simply add the following flag and value <code>-L 3000:127.0.0.1:3000</code> when connecting to create that SSH tunnel. After doing so, you will be able to connect to http://localhost:3000/ from your client machine, the one you are connecting from, to access Grafana. In the end, it might look like:</p> <pre><code>$ ssh -L 3000:127.0.0.1:3000 [user@]hostname[:port]\n</code></pre>"},{"location":"monitoring/#whats-next","title":"What's next?","text":"<p>You might want to add alerting to your setup. If so, check out my other Guide on how to do alerting for an Ethereum validator.</p>"},{"location":"monitoring/#support","title":"Support","text":"<p>If you have any question or if you need additional support, make sure to get in touch with the ethstaker community on:</p> <ul> <li>Discord: dsc.gg/ethstaker</li> <li>Reddit: reddit.com/r/ethstaker</li> </ul>"},{"location":"monitoring/#credits","title":"Credits","text":"<p>Based on Somer Esat's guide.</p>"},{"location":"pectra-features/","title":"New Staking Features in Pectra","text":"<p>Pectra, combining changes on the consensus layer (Electra) with changes on the execution layer (Prague), is the next hard fork planned to upgrade the Ethereum protocol. This document covers the new staking-related features and changes included in this upgrade.</p>"},{"location":"pectra-features/#fork-schedule","title":"Fork Schedule","text":"<p>Pectra was officially announced on Holesky and Sepolia. Here is the planned schedule for these forks:</p> <ul> <li>Holesky at epoch 115968 (Feb. 24, 2025, 21:55 UTC)</li> <li>Sepolia at epoch 222464 (Mar. 5, 2025, 7:29 UTC)</li> </ul> <p>It was officially announced on Mainnet. Here is the planned schedule:</p> <ul> <li>At epoch 364032 (May 07, 2025, 10:05:11 UTC)</li> </ul>"},{"location":"pectra-features/#consolidated-or-compounding-validators","title":"Consolidated or Compounding Validators","text":"<p>EIP-7251: Increase the MAX_EFFECTIVE_BALANCE defines a new type of validator called a consolidated or compounding validator. It enables stakers to earn compounding rewards on a single validator that can have up to 2048 effective ETH on its balance. This can be referred to as a type 2 or <code>0x02</code> validator, with <code>0x02</code> being the first 2 bytes of its withdrawal credentials. Previously, only type 0 (<code>0x00</code>) and type 1 (<code>0x01</code>) validators existed. Here are all the validator types available after Pectra:</p> <ul> <li>Type 0 or <code>0x00</code>: A regular validator without a withdrawal address. It can also be called a BLS or locked validator. Its balance continues to increase until it is converted to a type 1 validator.</li> <li>Type 1 or <code>0x01</code>: A regular validator with a withdrawal address. Its balance is capped at 32 ETH, after which an automatic partial withdrawal sends any excess balance to the withdrawal address on a rolling window (typically every few days).</li> <li>Type 2 or <code>0x02</code>: A compounding validator with a withdrawal address. Its balance is capped at 2048 ETH, after which an automatic partial withdrawal sends any excess balance to the withdrawal address on a rolling window. The rewards structure and slashing penalties are adjusted to be compounding and roughly equivalent to running multiple regular validators without the associated computational burden for the network or node operator.</li> </ul>"},{"location":"pectra-features/#rewards-and-proposer-selection-probability","title":"Rewards and Proposer Selection Probability","text":"<p>Unlike traditional validators, type 2 validators allow compounding rewards, meaning that earned rewards remain within the validator\u2019s balance instead of being automatically withdrawn above 32 ETH. This compounding effect enables long-term validators to accumulate higher returns over time.  </p> <p>However, the fundamental reward structure remains unchanged\u2014attestation, proposal, and sync committee rewards are calculated the same way as for type 1 validators. The probability of being selected as a block proposer or sync committee member is directly proportional to the validator\u2019s effective balance. This means that while a single type 2 validator with 2048 ETH has a higher probability of proposing a block compared to a 32 ETH validator, splitting the same 2048 ETH across multiple 32 ETH validators results in an identical overall proposal probability.  </p> <p>Additionally, slashing penalties have been reduced, making large validators less risky than before. Previously, slashing resulted in an immediate loss of 1/32 of a validator's balance, but with EIP-7251, this penalty has been reduced to 1/4096, significantly lowering the downside for high-balance validators.</p> <p>If you currently run a type 0 or type 1 validator, nothing will change after Pectra. Type 1 validators will continue receiving automatic partial withdrawals for balances above 32 ETH. You will have the option to use the new type 2 validator if desired.</p> <p>New validators can be deposited directly as type 2 validators. There is also a migration path from type 1 to type 2 validators through the consolidation request operation. This operation allows stakers to consolidate one or multiple validators into one or many larger type 2 validators. The operation is performed on the execution layer with a transaction sent from the validator withdrawal address to a smart contract. There are two types of transactions possible with the consolidation request operation:</p> <ol> <li>Transform an existing type 1 validator to a type 2 validator</li> <li>Transfer the balance from a type 1 or type 2 validator to a type 2 validator. This will exit the source validator and consolidate its balance on the target validator</li> </ol> <p>For a consolidation to work, the validator you're moving from must still be active (not exited), and the request must come from the same wallet that controls the withdrawal address of that validator. This is to ensure that only the person who owns the validator can approve moving its balance. The withdrawal address of the target validator (the one being consolidated into) does not have to match the withdrawal address of the source validator (the one being converted).</p> <p>While it has always been possible to deposit additional funds into an existing validator, this practice will likely become more common with the new compounding validator type. Previously, 32 ETH was the only possible maximum effective ETH balance, and most well-behaved validators' balances stayed near 32 ETH. After Pectra, we will see a wider distribution of validator balances on the consensus layer. Every integer increment in your validator's balance increases your potential rewards. For type 0 and type 1 validators, this caps at 32 ETH, but for type 2 validators, it caps at 2048 ETH. For example, if you have a type 2 validator with 44 ETH and want to stake an additional ETH, you can deposit it to increase your rewards. New deposits or top-ups must include at least 1 ETH to be valid.</p> <p>Various delays apply after performing a consolidation request operation, depending on the transaction type. The network defines a maximum of 2 consolidation requests per block, with 1 being the target. To avoid hindering and prevent others from using this feature a fee is used for rate limiting. It gets exponentially more expensive to add more consolidation requests as the network processes more than 1 per block. Requests are added to a queue and they are processed in order on the next block with available room.</p>"},{"location":"pectra-features/#user-triggered-exit-and-withdrawals","title":"User-Triggered Exit and Withdrawals","text":"<p>EIP-7002: Execution layer triggerable withdrawals defines new mechanisms allowing stakers to manually exit or withdraw any valid amount from their validator balance. These operations are performed on the execution layer via a transaction sent from the validator withdrawal address to a smart contract.</p> <p>Manual withdrawal requests are only possible with type 2 validators defined in EIP-7251. You can specify any withdrawal amount, but the operation will ensure at least 32 ETH remains in your validator's balance to prevent unexpected exits due to insufficient funds.</p> <p>Manual exit requests are available for both type 1 and type 2 validators. When requested, a validator will exit similarly to a voluntary exit performed on the consensus layer. While voluntary exits on the consensus layer are free, manual exit requests require the owner to pay gas fees and rate limiting fees.</p> <p>For these requests to work, the validator must be active on the consensus layer, and the withdrawal address must match the address sending the transaction.</p> <p>Various delays apply after performing an exit or withdrawal using this new mechanism. The network defines a maximum of 16 exit or withdrawal requests per block, with 2 being the target. To avoid hindering and prevent others from using this feature a fee is used for rate limiting. It gets exponentially more expensive to add more exit or withdrawal requests as the network processes more than 2 per block. Requests are added to a queue and they are processed in order on the next block with available room.</p>"},{"location":"pectra-features/#more-blobs","title":"More Blobs","text":"<p>EIP-7691: Blob throughput increase increases the number of blobs to scale Ethereum via L2 solutions. It raises the target number of blobs per block from 3 to 6 and increases the maximum number of blobs per block from 6 to 9.</p> <p>This will likely increase bandwidth requirements for stakers until PeerDAS is included in a later fork.</p>"},{"location":"pectra-features/#faster-deposits","title":"Faster deposits","text":"<p>EIP-6110: Supply validator deposits on chain improves the validator deposit mechanism on the beacon chain. This change reduces the delay between making a deposit and its recognition by the beacon chain from over 9 hours to approximately 13 minutes.</p>"},{"location":"pectra-features/#support","title":"Support","text":"<p>If you have questions or need additional support, connect with the EthStaker community on:</p> <ul> <li>Discord: dsc.gg/ethstaker</li> <li>Reddit: reddit.com/r/ethstaker</li> </ul>"},{"location":"prepare-for-the-merge/","title":"Guide on how to prepare a staking machine for the Merge","text":"<p>The Merge is a protocol upgrade that is being rolled out on all Ethereum networks being kept around. This protocol upgrade will be rolled out on Mainnet. Ethereum validators will need to take some measures and some actions to make sure their staking machine will work during this upgrade and that they will keep running well after the upgrade. Tests are already underway and the Merge upgrade has been applied to testnets already.</p> <p>This guide is meant for people who are managing and maintaining staking machines. It will explain which steps or actions you should take in order to be ready for the Merge.</p>"},{"location":"prepare-for-the-merge/#ethereum-client-updates-are-mandatory-for-the-merge-on-mainnet","title":"Ethereum client updates are mandatory for the Merge on Mainnet","text":"<p>Here are the clients that are ready for the merge on Mainnet and their version. You must update your clients to these versions or later before the Merge on Mainnet. You must update your clients before September 6th 2022 at 11:34:47am UTC.</p>"},{"location":"prepare-for-the-merge/#execution-clients-that-are-ready-for-the-merge-on-mainnet","title":"Execution clients that are ready for the Merge on Mainnet","text":"<ul> <li>Geth: v1.10.23 (or later)</li> <li>Nethermind: v1.14.0 (or later)</li> <li>Besu: v22.7.1 (or later)</li> <li>Erigon: v2022.08.03-alpha (or later)</li> </ul>"},{"location":"prepare-for-the-merge/#consensus-clients-that-are-ready-for-the-merge-on-mainnet","title":"Consensus clients that are ready for the Merge on Mainnet","text":"<ul> <li>Teku: v22.8.1 (or later)</li> <li>Lighthouse: v3.0.0 (or later)</li> <li>Prysm: v3.0.0 (or later)</li> <li>Nimbus: v22.8.1 (or later)</li> <li>Lodestar: v1.0.0 (or later)</li> </ul>"},{"location":"prepare-for-the-merge/#why-would-you-want-to-prepare-for-the-merge","title":"Why would you want to prepare for the Merge?","text":"<p>There are some risks your staking machine will stop performing its duties if you are not well prepared. There are some risks your Ethereum validator will miss some rewards if you are not well prepared.</p>"},{"location":"prepare-for-the-merge/#when-is-the-merge-going-to-happen-on-mainnet","title":"When is the Merge going to happen on Mainnet?","text":"<p>The Merge on Mainnet is happening in a few different steps. The first one will happen on September 6th 2022 at 11:34:47am UTC. It's a fork called Bellatrix to prepare for the Merge. The final and definitive step will happen around September 15th 2022. The uncertainty around the exact date and time depends in part on the current miners and the overall Ethereum network hash rate.</p> <p>Make sure to follow the official Ethereum blog and watch for announcements there. You can also subscribe to this newsletter.</p>"},{"location":"prepare-for-the-merge/#overview","title":"Overview","text":"<ol> <li>Make sure you are running your own execution client.</li> <li>Make sure your Ethereum clients are updated.</li> <li>Make sure you are using the new configuration options for your Ethereum clients.</li> <li>Make sure you have a configured JWT token file for the engine API.</li> <li>Make sure you have a configured fee recipient address.</li> <li>Make sure you have a configured MEV solution if you want one.</li> </ol>"},{"location":"prepare-for-the-merge/#running-your-own-execution-client","title":"Running your own execution client","text":"<p>Running your own execution client is going to be required after the Merge for an Ethereum validator. Some Ethereum validators are entirely relying on public infrastructure like Infura or Alchemy for their execution data. If you are in this case, you will need to install and configure your own execution client.</p> <p>If you are running your own execution client and using a public infrastructure provider as a fallback endpoint, that fallback endpoint will not work after the merge. Using multiple execution clients after the Merge is going to be slightly more complex. This is out of scope for this guide.</p> <p>The 2 main concerns for an Ethereum validator when running your own execution client is disk usage and bandwidth usage.</p>"},{"location":"prepare-for-the-merge/#installing-and-configuring-a-new-execution-client","title":"Installing and configuring a new execution client","text":"<p>If you are not currently running your own execution client and if you used a guide or a tool to setup your staking machine, you should first check to see if that guide or tool has a section on how to setup an execution client. If you don't know where to start, check out the client setup tools and guides. Our support can also help you here.</p> <p>Syncing an execution from nothing can take a few hours or a few days. Make sure to install and configure your execution client at least a week before the Merge to be safe.</p>"},{"location":"prepare-for-the-merge/#managing-your-execution-client-disk-usage","title":"Managing your execution client disk usage","text":"<p>It is not uncommon for an execution client to use more than 650GB to store its data on Mainnet. If your disk size is less than 2TB for storing everything, you should consider migrating and replacing that disk with one which is at least 2TB in size. You could possibly still manage to run an effective Ethereum validator with only a 1TB disk but that is going to require a lot of efforts which might not be worth it for the price difference of a new disk. Alternatives include adding another a new disk to your machine if it has an available slot or running your execution client on another machine on your local network. Make sure to get in touch with our support if you are interested in those alternatives.</p> <p>Pruning your execution client database is a good strategy for managing your execution client disk usage. If you are using Geth, you should prune it regularly and you should prune it just before the Merge. Geth currently only offers offline pruning that takes a few hours to complete. You should know that having an offline execution client after the Merge will result in your validator failing to perform all its duties meaning that you will lose rewards during that time.</p> <p>Other execution clients have different strategies for managing data growth. Some are leaner in terms of disk size usage to get started with and some include automatic online pruning. You should consider running a minority execution client.</p> <p>I suggest you implement monitoring and alerting to help you find out when you are low on available disk space.</p>"},{"location":"prepare-for-the-merge/#managing-your-execution-client-bandwidth-usage","title":"Managing your execution client bandwidth usage","text":"<p>An execution client will use a lot of bandwidth. Reducing the number of peers is one way of reducing bandwidth usage. It comes with some risks and the con of potentially being out of sync at the wrong time and making your Ethereum validator lose some rewards. The default configuration option for the number of peers should be your baseline value.</p>"},{"location":"prepare-for-the-merge/#execution-client-diversity","title":"Execution client diversity","text":"<p>Geth is currently the supermajority execution client. It is used to build almost all blocks right now. This is not currently a big issue but it will become one after the Merge. You should consider running a minority execution client. We have some great execution clients that work well for an Ethereum validator that should be considered:</p> <ul> <li>Erigon (Discord)</li> <li>Besu (Discord)</li> <li>Nethermind (Discord)</li> </ul>"},{"location":"prepare-for-the-merge/#updating-your-ethereum-clients","title":"Updating your Ethereum clients","text":"<p>There are new specification changes that were added recently that will be needed for the Merge. If you have not updated your Ethereum clients regularly, you will need to update them before the Merge.</p> <p>If you used a guide or a tool to setup your staking machine, you should first check out with that guide or tool on how you can update your Ethereum clients. If they don't have any section on how to do it, you can check with each client or get touch with our support.</p>"},{"location":"prepare-for-the-merge/#execution-clients","title":"Execution clients","text":"<ul> <li>Geth (Discord)</li> <li>Erigon (Discord)</li> <li>Besu (Discord)</li> <li>Nethermind (Discord)</li> </ul>"},{"location":"prepare-for-the-merge/#consensus-clients","title":"Consensus clients","text":"<ul> <li>Prysm (Discord)</li> <li>Nimbus (Discord)</li> <li>Lodestar (Discord)</li> <li>Teku (Discord)</li> <li>Lighthouse (Discord)</li> </ul>"},{"location":"prepare-for-the-merge/#using-the-new-configuration-options-for-your-ethereum-clients","title":"Using the new configuration options for your Ethereum clients","text":"<p>The communication between your consensus client and your execution client is going to change with the Merge. They will use this new Engine API which will be served at a port independent from the JSON-RPC API currently in use.</p> <p>You are likely going to need to use a new configuration option for this endpoint or at least update it on your consensus client beacon node. The default port for the Engine API is 8551 and the default port for the JSON-RPC API is 8545. You might not need to change any configuration option on your execution client, but you will likely need to modify your beacon node configuration. Here are the common values you will likely need to add or modify for each consensus client assuming your execution client engine API endpoint is at <code>http://localhost:8551</code>:</p> <ul> <li>Prysm beacon node: <code>--execution-endpoint http://localhost:8551</code></li> <li>Nimbus: <code>--web3-url=http://localhost:8551</code></li> <li>Lodestar beacon node: <code>--execution.urls http://localhost:8551</code></li> <li>Teku: <code>--ee-endpoint http://localhost:8551</code></li> <li>Lighthouse beacon node: <code>--execution-endpoint http://localhost:8551</code></li> </ul> <p>As usual, when changing the configuration for your consensus client, you will need to reload this configuration and probably restart the client.</p>"},{"location":"prepare-for-the-merge/#configuring-a-jwt-token-file","title":"Configuring a JWT token file","text":"<p>The new Engine API used to communicate between the execution client and the consensus client requires authentication which is provided with a JWT token stored in a file. There are various ways to configure this. Here is a simple one.</p>"},{"location":"prepare-for-the-merge/#creating-the-jwt-token-file","title":"Creating the JWT token file","text":"<p>Create a JWT token file in a neutral location and make it readable to everyone. We will use the <code>/var/lib/ethereum/jwttoken</code> location to store the JWT token file.</p> <pre><code>$ sudo mkdir -p /var/lib/ethereum\n$ openssl rand -hex 32 | tr -d \"\\n\" | sudo tee /var/lib/ethereum/jwttoken\n$ sudo chmod +r /var/lib/ethereum/jwttoken\n</code></pre>"},{"location":"prepare-for-the-merge/#configure-your-execution-client-to-use-the-jwt-token-file","title":"Configure your execution client to use the JWT token file","text":"<p>Each execution client will need a different configuration option for the JWT token file. Here are the common values:</p> <ul> <li>Geth: <code>--authrpc.jwtsecret /var/lib/ethereum/jwttoken</code></li> <li>Erigon: <code>--authrpc.jwtsecret /var/lib/ethereum/jwttoken</code></li> <li>Besu: <code>--engine-jwt-secret=/var/lib/ethereum/jwttoken</code></li> <li>Besu on mainnet: <code>--engine-rpc-enabled --engine-jwt-secret=/var/lib/ethereum/jwttoken</code></li> <li>Nethermind: <code>--JsonRpc.JwtSecretFile=/var/lib/ethereum/jwttoken</code></li> </ul> <p>As usual, when changing the configuration for your execution client, you will need to reload this configuration and probably restart the client.</p> <p>Besu requires <code>--engine-rpc-enabled</code> when no TTD has been configured for the network. Once mainnet has an announced TTD, this flag is no longer necessary</p>"},{"location":"prepare-for-the-merge/#configure-your-consensus-client-to-use-the-jwt-token-file","title":"Configure your consensus client to use the JWT token file","text":"<ul> <li>Prysm beacon node: <code>--jwt-secret /var/lib/ethereum/jwttoken</code></li> <li>Nimbus: <code>--jwt-secret=/var/lib/ethereum/jwttoken</code></li> <li>Lodestar beacon node: <code>--jwt-secret /var/lib/ethereum/jwttoken</code></li> <li>Teku: <code>--ee-jwt-secret-file /var/lib/ethereum/jwttoken</code></li> <li>Lighthouse beacon node: <code>--execution-jwt /var/lib/ethereum/jwttoken</code></li> </ul> <p>As usual, when changing the configuration for your consensus client, you will need to reload this configuration and probably restart the client.</p>"},{"location":"prepare-for-the-merge/#configuring-your-fee-recipient-address","title":"Configuring your fee recipient address","text":"<p>After the Merge, validators will replace miners in building blocks and getting the associated rewards. The validators will receive the tips from each included transaction on their proposed block. For this, they will need to configure a fee recipient address. This will represent the address where tips are sent to when your validator propose a block. Those fees will be available right away for any transaction.</p> <p>This address can be configured at different layers depending on which client you are using. It can be at the execution client layer, at the consensus beacon node layer, it can be at the consensus validator client layer and it can even be for each different validator key. Here are the common values to configure this on the consensus client. Make sure to replace the <code>0x0000000000000000000000000000000000000000</code> address with your own Ethereum address that you control where you want to receive the transaction tips.</p> <ul> <li>Prysm: <code>--suggested-fee-recipient 0x0000000000000000000000000000000000000000</code></li> <li>Nimbus: <code>--suggested-fee-recipient=0x0000000000000000000000000000000000000000</code></li> <li>Lodestar validator: <code>--suggestedFeeRecipient 0x0000000000000000000000000000000000000000</code></li> <li>Teku: <code>--validators-proposer-default-fee-recipient=0x0000000000000000000000000000000000000000</code></li> <li>Lighthouse: <code>--suggested-fee-recipient 0x0000000000000000000000000000000000000000</code></li> </ul> <p>As usual, when changing the configuration for your consensus client, you will need to reload this configuration and probably restart the client.</p> <p>There are some privacy implications in using a fee recipient address. That address will be forever linked with your validator when it proposes a block after the Merge. If you want to maximize your privacy, you should use your validator deposit address as your fee recipient address if you still have control over it since it is already linked with your validator.</p> <p>Here are the detailed configuration options for the fee recipient for each client and their documentation.</p> <ul> <li>Prysm</li> <li>Nimbus</li> <li>Lodestar</li> <li>Teku</li> <li>Lighthouse</li> </ul>"},{"location":"prepare-for-the-merge/#choosing-and-configuring-an-mev-solution","title":"Choosing and configuring an MEV solution","text":"<p>Maximal extractable value (MEV) refers to the maximum value that can be extracted from block production in excess of the standard block reward and gas fees by including, excluding, and changing the order of transactions in a block. After the Merge, there will be an opportunity for Ethereum validators to get that value. If you want to get that value, you will need to install some additional software and configure your consensus client to communicate with that software when the time comes to propose a block.</p> <p>The use of mev-boost and obtaining MEV by a validator is entirely optional. A validator might want to avoid obtaining MEV for any reason. There are plenty of good reasons to avoid MEV. It is a decision left to each staker. However, if you want to maximize your profits, you should consider obtaining that additional value.</p> <p>A preview of MEV and mev-boost can be seen on https://youtu.be/sZYJiLxp9ow</p>"},{"location":"prepare-for-the-merge/#installing-mev-boost","title":"Installing mev-boost","text":"<p>Create a user account for the service to run under. This account will not be able to log into the machine. It will only be used to run the service.</p> <pre><code>$ sudo useradd --no-create-home --shell /bin/false mevboost\n</code></pre> <p>Download the latest stable version of mev-boost from https://github.com/flashbots/mev-boost/releases (avoid any pre-release version). As of this date, the latest stable release version is 1.9 . Adjust the following instructions accordingly if there is a newer stable release version with a different archive name. The file name should likely end with linux_amd64.tar.gz (for linux and AMD64 instructions set). Use a different archive if you are on a different architecture or a different operating system.</p> <pre><code>$ cd ~\n$ wget https://github.com/flashbots/mev-boost/releases/download/v1.9/mev-boost_1.9_linux_amd64.tar.gz\n</code></pre> <p>Verify that the SHA256 Checksum as shown in the checksums.txt file is the same as the file we just downloaded.</p> <pre><code>$ sha256sum mev-boost_1.9_linux_amd64.tar.gz\n</code></pre> <p>Extract the archive. Install mev-boost globally and remove the download leftovers.</p> <pre><code>$ tar xvf mev-boost_1.9_linux_amd64.tar.gz\n$ sudo cp mev-boost /usr/local/bin\n$ rm mev-boost LICENSE README.md mev-boost_1.9_linux_amd64.tar.gz\n$ sudo chown mevboost:mevboost /usr/local/bin/mev-boost\n</code></pre> <p>Create a systemd service file to store the service config which tells systemd to run mev-boost as the mevboost user.</p> <pre><code>$ sudo nano /etc/systemd/system/mevboost.service\n</code></pre> <p>Paste the following into the file to run mev-boost on Mainnet. You must replace <code>https://example.com</code> in this configuration with one or many existing relays. We have a list of relays you can explore. Exit and save once done (<code>Ctrl</code> + <code>X</code>, <code>Y</code>, <code>Enter</code>).</p> <pre><code>[Unit]\nDescription=mev-boost (Mainnet)\nWants=network-online.target\nAfter=network-online.target\n\n[Service]\nType=simple\nUser=mevboost\nGroup=mevboost\nRestart=always\nRestartSec=5\nExecStart=mev-boost \\\n    -mainnet \\\n    -min-bid 0.05 \\\n    -relay-check \\\n    -relays https://example.com\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <p>You can add multiple relays comma-separated to the <code>-relays</code> flag, like this: <code>-relays https://relay1,https://relay2</code>.</p> <p>You can change the min-bid value to reflect how much you want to use the MEV relays in relation to how much they are offering in terms of rewards. There is little value in giving all block production to MEV relays if they are not to offer any substancial rewards in exchange. The 0.05 ETH min-bid value is a good compromise. You can learn more on about this on The Cost of Resilience.</p> <p>Selecting your relays can be an important decision for some stakers. Some relay might sensor transactions. Some relay might be more ethical in terms of which strategies they accept or not. You should do your own diligence when selecting which relay you want to use. EthStaker has nice list of MEV relays to choose from.</p> <p>Reload systemd to reflect the changes. If you edit that file again to add or remove relays, you need will to reload systemd again to reflect the changes before restarting your mev-boost service.</p> <pre><code>$ sudo systemctl daemon-reload\n</code></pre> <p>And then start the service with the following command and check the status to make sure it\u2019s running correctly.</p> <pre><code>$ sudo systemctl start mevboost\n$ sudo systemctl status mevboost\n</code></pre> <p>If mev-boost crashes with <code>\"SIGILL: illegal instruction\"</code> then you need to use a portable build.</p> <p>If you did everything right, it should say active (running) in green. If not then go back and repeat the steps to fix the problem. Press Q to quit. Lastly, enable mev-boost to start on boot.</p> <pre><code>$ sudo systemctl enable mevboost\n</code></pre>"},{"location":"prepare-for-the-merge/#configure-your-consensus-and-validator-client-to-use-mev-boost","title":"Configure your consensus and validator client to use mev-boost","text":"<p>Add this flag to the <code>ExecStart</code> of your consensus client and, if needed, validator client service. Note that client flags can be in flux through August 2022. When in doubt consult each client's <code>--help</code>.</p> <ul> <li>Prysm consensus: <code>--http-mev-relay=http://127.0.0.1:18550</code></li> <li>Prysm validator: <code>--enable-builder</code></li> <li>Nimbus consensus: <code>--payload-builder=true --payload-builder-url=http://127.0.0.1:18550</code></li> <li>Nimbus validator: <code>--payload-builder=true</code></li> <li>Lodestar consensus: <code>--builder --builder.urls http://127.0.0.1:18550</code></li> <li>Lodestar validator: <code>--builder</code></li> <li>Teku combined: <code>--validators-builder-registration-default-enabled=true --builder-endpoint=http://127.0.0.1:18550</code></li> <li>Lighthouse consensus: <code>--builder http://127.0.0.1:18550</code> </li> <li>Lighthouse validator: <code>--builder-proposals</code></li> </ul> <p>Tell systemd you made the changes: <code>sudo systemctl daemon-reload</code></p> <p>And restart the service(s) you changed: <code>sudo systemctl restart SERVICENAME</code></p>"},{"location":"prepare-for-the-merge/#update-mev-boost","title":"Update mev-boost","text":"<p>When a new version is released, you can update mev-boost. Find the latest stable version of mev-boost from https://github.com/flashbots/mev-boost/releases. Download the archive. The file name should likely end with linux_amd64.tar.gz (for linux and AMD64 instructions set). Use a different archive if you are on a different architecture or a different operating system. Verify the archive checksum, extract the archive, stop the service, install mev-boost globally, remove the download leftovers and restart the service.</p> <pre><code>$ cd ~\n$ wget https://github.com/flashbots/mev-boost/releases/download/v1.9/mev-boost_1.9_linux_amd64.tar.gz\n$ sha256sum mev-boost_1.9_linux_amd64.tar.gz\n$ tar xvf mev-boost_1.9_linux_amd64.tar.gz\n$ sudo systemctl stop mevboost\n$ sudo cp mev-boost /usr/local/bin\n$ rm mev-boost LICENSE README.md mev-boost_1.9_linux_amd64.tar.gz\n$ sudo chown mevboost:mevboost /usr/local/bin/mev-boost\n$ sudo systemctl start mevboost\n</code></pre>"},{"location":"prepare-for-the-merge/#proposing-a-block-with-an-mev-relay","title":"Proposing a block with an MEV relay","text":"<p>When proposing a block with an MEV relay, your validator will blindly sign a block and that block will be published by the relay itself or a builder behind it. The configured fee recipient address for your validator will not directly receive the associated rewards for using that relay. It will not be used for the actual fee recipient field for that block. Instead and in most cases, a separate transaction will be included in that block that will send your profit to the configured fee recipient address. That should be the case even if you have multiple different configured fee recipient addresses when running multiple validators. Make sure to look at the included transactions in your block to find a transaction with your MEV rewards. Make sure to contact the MEV operator support if you feel like you did not receive your share of the profit. Make sure to get in touch with EthStaker if you feel like your did not receive your share of the profit.</p>"},{"location":"prepare-for-the-merge/#support","title":"Support","text":"<p>If you have any question or if you need additional support, make sure to get in touch with the EthStaker community on:</p> <ul> <li>Discord: dsc.gg/ethstaker</li> <li>Reddit: reddit.com/r/ethstaker</li> </ul>"},{"location":"prepare-for-the-merge/#credits","title":"Credits","text":"<p>Based on CoinCashew's Ethereum Merge Upgrade Checklist for Home Stakers and Validators.</p>"},{"location":"security-maintenance/","title":"Guide on how to do security and maintenance for an Ethereum validator's machine","text":"<p>Your staking machine is unsafe like many others, but we can help it. Just like with many things in life, you are consistently being exposed to risks you might not even know about. You can spend a very large amount of time, efforts and money and still be exposed to many risks. The rabbit hole goes deep. This guide is a first plunge into the world of security and maintenance to give you a foundation for your own Ethereum validator's machine.</p> <p>The following suggestions are general tips that can apply to many different environments and use cases. Security can be subjective. What feels safe or safer for someone can be quite different from someone else. As you start looking at these risks from different angles, you may find alternative solutions that have their own pros and cons.</p> <p>This is not an exhaustive list of all the security practices someone who runs a validator or who handles cryptocurrencies should use. We have a reference section with additional security practices you should be looking at as well.</p>"},{"location":"security-maintenance/#using-a-good-operating-system","title":"Using a good operating system","text":"<p>Risks</p> <ul> <li>Using an operating system that is hard to secure or hard to maintain.</li> <li>Using an operating system that has poor default configuration values.</li> <li>Using an operating system that has weak support or that has a weak community.</li> <li>Using an operating system that exposes you to unnecessary risks.</li> </ul> <p>If you do not know which operating system (OS) to use for your staking machine, use Ubuntu 22.04 Desktop. If you are familiar with the command line interface (CLI) and manually typing commands in a terminal, I suggest you use Ubuntu 22.04 Server. Ubuntu 22.04 is a long term support (LTS) release. It will be supported until 2032 which gives you a peace of mind.</p> <p>While you can use MacOS or Windows to run your staking machine, I would recommend against it. They will expose you to additional risks and they are harder to manage in terms of security and general maintenance.</p> <p>There are various other good Linux distributions that can work, but the rest of this guide will assume you are using Ubuntu 22.04. If you know what you are doing and you are familiar with Linux, you should still be able to follow even if you used another modern Linux distribution.</p> <p>Installing a modern Linux operating system on your own machine is often as simple as:</p> <ol> <li>Downloading the OS image.</li> <li>Copying the OS image on a USB drive and making it bootable. Rufus or Etcher are two good tools to accomplish this.</li> <li>Plugging in that USB drive, rebooting your machine and booting your machine from that USB drive. That last part can be somewhat tricky depending on your boot sequence and your motherboard. On many modern PC, you can press and hold the <code>F2</code> key on your keyboard after a reboot to enter your BIOS. From there, you can select on which device or drive to boot from or you can change the boot sequence order. On Mac, you can press and hold the Option (<code>\u2325</code>) key immediately upon hearing the startup chime to enter the Startup Manager and select which device or drive to boot from. In case of doubts, refer to your machine manual, your motherboard manual or get in touch with the ETHStaker community.</li> </ol>"},{"location":"security-maintenance/#using-a-dedicated-machine","title":"Using a dedicated machine","text":"<p>Risks</p> <ul> <li>Exposing your machine to unrelated daily usage risks.</li> <li>Starving your staking machine resources.</li> <li>Unexpected or inopportune machine reboots.</li> </ul>"},{"location":"security-maintenance/#securing-your-remote-access","title":"Securing your remote access","text":"<p>Risks</p> <ul> <li>Unauthorized or unintended remote access to your machine.</li> </ul> <p>If you are using SSH to remotely access your machine, you should configure your server to authenticate with keys and disable password authentication.</p>"},{"location":"security-maintenance/#securing-your-network-with-a-firewall","title":"Securing your network with a firewall","text":"<p>Risks</p> <ul> <li>Unauthorized or unintended remote access to your machine.</li> </ul>"},{"location":"security-maintenance/#securing-the-machine-you-use-to-remote-connect-with","title":"Securing the machine you use to remote connect with","text":"<p>Risks</p> <ul> <li>Unauthorized or unintended remote access to your machine.</li> </ul>"},{"location":"security-maintenance/#performing-your-system-updates-regularly","title":"Performing your system updates regularly","text":""},{"location":"security-maintenance/#performing-your-application-updates-regularly","title":"Performing your application updates regularly","text":""},{"location":"security-maintenance/#using-a-live-patching-service","title":"Using a live patching service","text":"<p>Risks</p> <ul> <li>Using a kernel that has vulnerabilities between updates or reboots.</li> <li>Missing important staking rewards during reboots.</li> </ul> <p>Ubuntu and Canonical offers a live patching service called Livepatch which is free for up to 3 machines.</p>"},{"location":"security-maintenance/#limiting-the-installed-applications-and-running-processes","title":"Limiting the installed applications and running processes","text":""},{"location":"security-maintenance/#limiting-the-users-who-can-access-your-machine","title":"Limiting the users who can access your machine","text":""},{"location":"security-maintenance/#using-disk-encryption","title":"Using disk encryption","text":"<p>Risks</p> <ul> <li>Physical data theft</li> <li>Unintended slashing</li> </ul>"},{"location":"security-maintenance/#good-security-and-maintenance-references","title":"Good security and maintenance references","text":"<ul> <li>Protecting Yourself and Your Funds by Jennicide and MyCrypto.</li> <li>Security Best Practices for a ETH staking validator node by CoinCashew.</li> <li>Ethereum 2.0 Node Security Discussion by CryptoManufaktur.</li> <li>MyCrypto\u2019s Security Guide For Dummies And Smart People Too by MyCrypto.</li> <li>Guide: Crypto Wallet Tips 101 - Do's and Don'ts by CoinCashew.</li> </ul>"},{"location":"security-maintenance/#support","title":"Support","text":"<p>If you have any question or if you need additional support, make sure to get in touch with the ETHStaker community on:</p> <ul> <li>Discord: dsc.gg/ethstaker</li> <li>Reddit: reddit.com/r/ethstaker</li> </ul>"},{"location":"voluntary-exit/","title":"Guide on how to perform a voluntary exit for your validator","text":"<p>At some point during a normal validator's lifecycle, you will want to exit and potentially obtain your balance back, the initial deposit plus any remaining rewards, assuming you have a withdrawal address associated with your validator. This document will guide you through some steps to perform a voluntary exit of your validator.</p> <p>This guide is meant for people who are staking on Ethereum either through their own node or with a non-custodial service. It will explain in details which steps or actions you should take in order to perform a voluntary exit.</p> <p>There are a lot of different ways to perform a voluntary exit. Each consensus client has their own documentation on this. This guide will focus on using ethdo and the beaconcha.in website to perform this task for simplicity. Check out the documentation for each client to learn more about how to do this natively.</p> <ul> <li>Prysm</li> <li>Nimbus</li> <li>Lodestar</li> <li>Teku</li> <li>Lighthouse</li> </ul> <p>A video tutorial of this guide can be seen on https://youtu.be/KoBAacMWA_k .</p>"},{"location":"voluntary-exit/#overview","title":"Overview","text":"<p>We will use 3 tools with this guide: ethdo, the beaconcha.in broadcast tool and Tails.</p> <p>ethdo is a command-line tool for managing common tasks in Ethereum. In this guide, we will use it to sign your voluntary exit from all the validator details we will provide.</p> <p>beaconcha.in is an open-source Ethereum explorer. In this guide, we will use it to broadcast the voluntary exit using their Broadcast Signed Messages tool.</p> <p>Tails is a portable operating system that protects against surveillance and malicious actors. In this guide, we will use it to perform all the sensitive operations offline to prevent exposing your secrets.</p> <p>The first step will be to prepare and generate a file for offline signing using ethdo. The second step will be to boot a computer into Tails, sign and generate your voluntary exit file using ethdo. The third step will be to broadcast this voluntary exit. You will need 2 USB sticks, one to boot the Tails operating system from and the second one to hold ethdo and the documents we will need. You will also need a PC that supports AMD64 instructions to boot into.</p>"},{"location":"voluntary-exit/#required-details","title":"Required details","text":"<p>To perform a voluntary exit, you need the validator signing key. This can be obtained through 2 main ways, either with your mnemonic, the 24 words used to create your validator keys, or with the keystore file and the associated password. If you do not have either of them, you are likely out of luck and you are unlikely to ever be able to perform a voluntary exit.</p> <p>You will also need the validator public key or the validator index as identified by the beacon chain. Both of these can be found on the beaconcha.in website. You can find your validator's page by searching by public key, deposit address, validator index or a few other ways. From there, you should be able to find your validator index or public key at the top of the validator's page.</p> <p>NEVER enter your mnemonic into a machine that was or will be online. This would potentially expose you to a malicious actor stealing your money.</p>"},{"location":"voluntary-exit/#tooling","title":"Tooling","text":"<p>For most of this guide, we will use ethdo version 1.37.3. You should use the latest stable version available from https://github.com/wealdtech/ethdo/releases and adapt this guide to that latest version if a new version is released.</p>"},{"location":"voluntary-exit/#preparing-for-offline-generation","title":"Preparing for offline generation","text":"<p>Using ethdo, we will connect to a beacon node endpoint and create a file named <code>offline-preparation.json</code> to be used later. If you do not have direct access to your own beacon node endpoint, ethdo will fallback using its own public endpoint assuming you are connected to the internet.</p>"},{"location":"voluntary-exit/#on-windows","title":"On Windows","text":"<p>Open a powershell command prompt, press <code>\u229e Win</code>+<code>R</code>, type <code>powershell</code> and press <code>\u21b5 Enter</code>. You will see a blue or black window where you can type commands.</p> <p>Download the ethdo archive and the associated checksum file.</p> <pre><code>$ProgressPreference = 'SilentlyContinue'; iwr https://github.com/wealdtech/ethdo/releases/download/v1.37.3/ethdo-1.37.3-windows-exe.zip -outfile ethdo-1.37.3-windows-exe.zip\n$ProgressPreference = 'SilentlyContinue'; iwr https://github.com/wealdtech/ethdo/releases/download/v1.37.3/ethdo-1.37.3-windows-exe-zip.sha256 -outfile ethdo-1.37.3-windows-exe-zip.sha256\n</code></pre> <p>Compute the archive hash value and compare it to the expected value.</p> <pre><code>Get-FileHash .\\ethdo-1.37.3-windows-exe.zip | Select -Property @{n='hash';e={$_.hash.tolower()}} | Select -ExpandProperty \"hash\"\ncat .\\ethdo-1.37.3-windows-exe-zip.sha256\n</code></pre> <p>Both of these output values should match. As of today and for version <code>1.37.3</code>, they should both be <code>75dcf86730d02a65eabce40a4d4e7f1d2f6f896ec05678f6618f41484514ce4b</code>. If they do not match, there might be a security issue and you should seek further support.</p> <p>Extract the ethdo archive and generate the preparation file.</p> <pre><code>Expand-Archive .\\ethdo-1.37.3-windows-exe.zip\ncd .\\ethdo-1.37.3-windows-exe\\\n.\\ethdo.exe validator exit --prepare-offline\n</code></pre> <p>You should see a message saying offline-preparation.json generated if everything worked fine.</p> <p>Download the Linux version of ethdo as well to prepare for when we will need to execute this offline on Tails.</p> <pre><code>$ProgressPreference = 'SilentlyContinue'; iwr https://github.com/wealdtech/ethdo/releases/download/v1.37.3/ethdo-1.37.3-linux-amd64.tar.gz -outfile ethdo-1.37.3-linux-amd64.tar.gz\n$ProgressPreference = 'SilentlyContinue'; iwr https://github.com/wealdtech/ethdo/releases/download/v1.37.3/ethdo-1.37.3-linux-amd64.tar.gz.sha256 -outfile ethdo-1.37.3-linux-amd64.tar.gz.sha256\n</code></pre> <p>Open an explorer window in this current directory to easily access the files we just obtained.</p> <pre><code>start .\n</code></pre>"},{"location":"voluntary-exit/#on-macos","title":"On macOS","text":"<p>Open a terminal. Download the etho archive and the associated checksum file.</p> <pre><code>curl -O -L https://github.com/wealdtech/ethdo/releases/download/v1.37.3/ethdo-1.37.3-darwin-amd64.tar.gz\ncurl -O -L https://github.com/wealdtech/ethdo/releases/download/v1.37.3/ethdo-1.37.3-darwin-amd64.tar.gz.sha256\n</code></pre> <p>Make sure ethdo checksum matches.</p> <pre><code>shasum -a 256 ethdo-1.37.3-darwin-amd64.tar.gz\ncat ethdo-1.37.3-darwin-amd64.tar.gz.sha256\n</code></pre> <p>Both of these output values should have matching checksum. As of today and for version <code>1.37.3</code>, they should both be <code>8134a99ca8192a5f3c24329c4560c2f366cf3204bddfd712b02bce7016b1fa45</code>. If they do not match, there might be a security issue and you should seek further support.</p> <p>Extract the ethdo archive and generate the preparation file.</p> <pre><code>tar xvf ethdo-1.37.3-darwin-amd64.tar.gz\n./ethdo validator exit --prepare-offline\n</code></pre> <p>You should see a message saying offline-preparation.json generated if everything worked fine.</p> <p>Download the Linux version of ethdo as well to prepare for when we will need to execute this offline on Tails.</p> <pre><code>curl -O -L https://github.com/wealdtech/ethdo/releases/download/v1.37.3/ethdo-1.37.3-linux-amd64.tar.gz\ncurl -O -L https://github.com/wealdtech/ethdo/releases/download/v1.37.3/ethdo-1.37.3-linux-amd64.tar.gz.sha256\n</code></pre> <p>Open a finder window in this current directory to easily access the files we just obtained.</p> <pre><code>open .\n</code></pre>"},{"location":"voluntary-exit/#on-linux","title":"On Linux","text":"<p>Open a terminal. Download the etho archive and the associated checksum file.</p> <pre><code>wget https://github.com/wealdtech/ethdo/releases/download/v1.37.3/ethdo-1.37.3-linux-amd64.tar.gz\nwget https://github.com/wealdtech/ethdo/releases/download/v1.37.3/ethdo-1.37.3-linux-amd64.tar.gz.sha256\n</code></pre> <p>Make sure ethdo checksum matches.</p> <pre><code>sha256sum ethdo-1.37.3-linux-amd64.tar.gz\ncat ethdo-1.37.3-linux-amd64.tar.gz.sha256\n</code></pre> <p>Both of these output values should have matching checksum. As of today and for version <code>1.37.3</code>, they should both be <code>c0f31fdea31d6d200f22dd261bc9d6b7d363c80f10aab7ee4d038e38deb7eecc</code>. If they do not match, there might be a security issue and you should seek further support.</p> <p>Extract the ethdo archive and generate the preparation file.</p> <pre><code>tar xvf ethdo-1.37.3-linux-amd64.tar.gz\n./ethdo validator exit --prepare-offline\n</code></pre> <p>You should see a message saying offline-preparation.json generated if everything worked fine.</p>"},{"location":"voluntary-exit/#prepared-alternative","title":"Prepared alternative","text":"<p>If you are having issues creating your <code>offline-preparation.json</code>, you can use these alternative files. They are regenerated every day at 0:00 UTC. You will need to extract the archive first (either the <code>.tar.gz</code> file on Linux or macOS or the <code>.zip</code> file on Windows) to get the <code>offline-preparation.json</code> file.</p> <p>For Mainnet: - https://files.ethstaker.cc/offline-preparation-mainnet.tar.gz - https://files.ethstaker.cc/offline-preparation-mainnet.tar.gz.sha256 - https://files.ethstaker.cc/offline-preparation-mainnet.zip - https://files.ethstaker.cc/offline-preparation-mainnet.zip.sha256</p> <p>For Hoodi: - https://files.ethstaker.cc/offline-preparation-hoodi.tar.gz - https://files.ethstaker.cc/offline-preparation-hoodi.tar.gz.sha256 - https://files.ethstaker.cc/offline-preparation-hoodi.zip - https://files.ethstaker.cc/offline-preparation-hoodi.zip.sha256</p>"},{"location":"voluntary-exit/#finalizing-your-documents","title":"Finalizing your documents","text":"<p>Copy those files on your second USB stick.</p> <ul> <li><code>offline-preparation.json</code></li> <li><code>ethdo-1.37.3-linux-amd64.tar.gz</code></li> <li><code>ethdo-1.37.3-linux-amd64.tar.gz.sha256</code></li> </ul> <p>If you want to use your keystore file and the associated password to generate your voluntary exit file, make sure to copy the keystore file on the same USB stick that you put the <code>offline-preparation.json</code> file. Try avoiding entering the associated password in a file on the same USB stick in clear text.</p> <p>If instead you want to use your mnemonic to generate your voluntary exit, avoid entering it in a file on your USB stick. Try to keep it away from any electronic document and any electronic medium. If you have it on a paper or in steel, keep it close as we will need it in the next step.</p>"},{"location":"voluntary-exit/#signing-and-generating-your-voluntary-exit-file","title":"Signing and generating your voluntary exit file","text":"<p>Install Tails on your first USB stick which should be empty by following the instructions from their website. Unplug any wired connection and restart a machine on your Tails USB stick. During start, you will be asked to select your language, keyboard layout and formats. Click Start Tails to reach the Desktop.</p> <p>Plug in your second USB stick and copy all the documents and tools we included in the home folder.</p>"},{"location":"voluntary-exit/#verifying-and-extracting-ethdo","title":"Verifying and extracting ethdo","text":"<p>Start a terminal. Make sure ethdo checksum matches.</p> <pre><code>sha256sum ethdo-1.37.3-linux-amd64.tar.gz\ncat ethdo-1.37.3-linux-amd64.tar.gz.sha256\n</code></pre> <p>Both of these output values should have matching checksum. As of today and for version <code>1.37.3</code>, they should both be <code>c0f31fdea31d6d200f22dd261bc9d6b7d363c80f10aab7ee4d038e38deb7eecc</code>. If they do not match, there might be a security issue and you should seek further support.</p> <p>Extract the ethdo archive.</p> <pre><code>tar xvf ethdo-1.37.3-linux-amd64.tar.gz\n</code></pre> <p>From here you can either use your keystore file and the associated password or use your mnemonic to generate your voluntary exit file.</p>"},{"location":"voluntary-exit/#generating-a-voluntary-exit-using-your-keystore-file-and-the-associated-password","title":"Generating a voluntary exit using your keystore file and the associated password","text":"<p>There are 3 inputs for the next command that will generate your voluntary exit file.</p> <ol> <li>Your keystore filename. This is going to be KEYSTORE_FILENAME in the template.</li> <li>Your keystore password. This is going to be KEYSTORE_PASSWORD in the template.</li> <li>The resulting filename. This is going to be RESULTING_FILENAME in the template.</li> </ol> <p>As a template, the command call looks like:</p> <pre><code>./ethdo validator exit --validator=\"KEYSTORE_FILENAME\" --passphrase='KEYSTORE_PASSWORD' --json --offline &gt; RESULTING_FILENAME\n</code></pre> <p>Here is a concrete example of using this command.</p> <pre><code>./ethdo validator exit --validator=\"keystore-m_12381_3600_0_0_0-1679368539.json\" --passphrase='testing123' --json --offline &gt; 459921-exit.json\n</code></pre> <p>In this example, it would result in a file named <code>459921-exit.json</code> in your home folder for performing the voluntary exit of the validator that is using the imported keystore file.</p> <p>Copy that resulting file back on your second USB stick. We will need it on the next step to broadcast the voluntary exit.</p>"},{"location":"voluntary-exit/#generating-a-voluntary-exit-using-your-mnemonic","title":"Generating a voluntary exit using your mnemonic","text":"<p>There are 3 inputs for the next command that will generate your voluntary exit file.</p> <ol> <li>Your validator index as identified on the beacon chain or your validator public key. This is going to be VALIDATOR_INDEX in the template.</li> <li>Your mnemonic. This is going to be MNEMONIC in the template.</li> <li>The resulting filename. This is going to be RESULTING_FILENAME in the template.</li> </ol> <p>As a template, the command call looks like:</p> <pre><code>./ethdo validator exit --validator=VALIDATOR_INDEX --json --offline --mnemonic=\"MNEMONIC\" &gt; RESULTING_FILENAME\n</code></pre> <p>Here is a concrete example of using this command.</p> <pre><code>./ethdo validator exit --validator=459921 --json --offline --mnemonic=\"silent hill auto ability front sting tunnel empower venture once wise local suffer repeat deny deliver hawk silk wedding random coil you town narrow\" &gt; 459921-exit.json\n</code></pre> <p>In this example, it would result in a file named <code>459921-exit.json</code> in your home folder for performing the voluntary exit of validator 459921.</p> <p>Copy that resulting file back on your second USB stick. We will need it on the next step to broadcast the voluntary exit.</p>"},{"location":"voluntary-exit/#broadcasting-your-voluntary-exit","title":"Broadcasting your voluntary exit","text":"<p>Shut down Tails and go back to your main connected machine. Plug your second USB stick. Browse to https://beaconcha.in/tools/broadcast, the beaconcha.in broadcast tool. Drag and drop or select your voluntary exit json file on that website from your USB stick. Click the Submit &amp; Broadcast button. </p> <p>Your validator will enter the exit queue and it will eventually fully exit the network. Your validator is expected to perform its duties even after you broadcast your voluntary exit. Make sure to let your validator run and perform its regular duties as long as it's not fully exited.</p>"},{"location":"voluntary-exit/#using-a-beacon-node-for-broadcasting","title":"Using a beacon node for broadcasting","text":"<p>If you have access to a beacon node, a consensus client currently in sync with an exposed beacon node API endpoint, you can manually submit your voluntary exit with that beacon node. That alternative process is usually harder than just using beaconcha.in broadcast tool. If you have a node running under Linux this can be done with the curl tool on the command line interface. You will need your voluntary exit file on the machine where you want to execute this command.</p> <p>There are 2 inputs for the next command that will broadcast your voluntary exit using a beacon node.</p> <ol> <li>Your beacon node API endpoint URL. This is going to be BEACON_NODE in the template.</li> <li>The path to your voluntary exit file. This is going to be EXIT_FILE in the template.</li> </ol> <p>As a template, the command call looks like:</p> <pre><code>curl -X POST BEACON_NODE/eth/v1/beacon/pool/voluntary_exits -H \"Content-Type: application/json\" -d @EXIT_FILE\n</code></pre> <p>Here is a concrete example of using this command.</p> <pre><code>curl -X POST http://localhost:5052/eth/v1/beacon/pool/voluntary_exits -H \"Content-Type: application/json\" -d @459921-exit.json\n</code></pre>"},{"location":"voluntary-exit/#full-exit-withdrawal-process","title":"Full exit / withdrawal process","text":"<p>There are multiple steps and delays when performing a voluntary exit and waiting for the final withdrawal. Check out this great graphic by Ladislaus to find more about those steps.</p>"},{"location":"voluntary-exit/#support","title":"Support","text":"<p>If you have any question or if you need additional support, make sure to get in touch with the ethstaker community on:</p> <ul> <li>Discord: dsc.gg/ethstaker</li> <li>Reddit: reddit.com/r/ethstaker</li> </ul>"},{"location":"home-router/comcast-bridge/","title":"Comcast Bridge Mode","text":"<p>You are ready to make the Netgear RS router your sole router, and switch the Comcast router to being a \"dumb modem\". This will disrupt network connectivity for all devices in your home.</p> <p>Hopefully, just for a few minutes.</p>"},{"location":"home-router/comcast-bridge/#switch-comcast-router-to-bridge-mode","title":"Switch Comcast router to bridge mode","text":"<ul> <li>Connect the Netgear RS router to one of the LAN (local network) ports on the Comcast router.</li> <li>Disconnect any other wired connections from the Comcast router - such as your node - and connect them to one of the LAN (local network) ports on the Netgear RS router. The 2.5G ones are nice if your node has 2.5G support, and don't hurt if it doesn't.</li> <li>Connect to the Comcast router via LAN or WiFi, navigate to <code>http://10.0.0.1</code>, and switch \"Bridge Mode\" to \"Enabled\". Comcast has instructions for this.</li> </ul> <p>In our testing, the Netgear RS router did not receive an external public IP right away. We power cycled the Comcast router/modem and the Netgear RS, and then it worked.</p> <p>You can see the external address at Advanced -&gt; Internet Port -&gt; IP Address. Netgear have documentation for this.</p> <p>A public IP is any that isn't private: Does not look like <code>192.168.x.x</code> or <code>10.x.x.x</code> or <code>172.16.x.x</code> through <code>172.31.x.x</code>.</p>"},{"location":"home-router/comcast-bridge/#configure-ipv6-on-netgear","title":"Configure IPv6 on Netgear","text":"<p>Comcast offers IPv6 services in the US, and we'd want that to continue working for you.</p> <p>On the Netgear web interface, go to Advanced -&gt; Advanced Setup -&gt; IPv6, and set \"Internet Connection Type\" to \"Auto Detect\", then click \"Apply\"</p> <p>You can verify this worked by coming back to that page and seeing that an IPv6 address has been assigned to the Netgear RS. Your devices on the network should also show that they have both an IPv4 and an IPv6 address.</p>"},{"location":"home-router/overview/","title":"Overview","text":""},{"location":"home-router/overview/#the-problem","title":"The Problem","text":"<p>When running an Ethereum node at home, the ISP's router may \"buckle\" under the sheer number of connections your node creates.</p> <p>The symptom for this is typically that Internet access slows to a crawl or fails entirely, but recovers for a while when the router is rebooted, and/or the Ethereum node is shut off.</p> <p></p> <p>Above: Typical scenario with undersized ISP router</p>"},{"location":"home-router/overview/#the-solution","title":"The Solution","text":"<p>Place the ISP's router into \"bridge mode\", and run your own router. The ISP's router in bridge mode no longer understands IP addresses, does not provide WiFi, and generally has no clue what a \"connection\" is.</p> <p>Your own router handles connections and WiFi, and will handle the Ethereum node with ease.</p> <p>The ISP's router is kept in the setup so that the ISP has a clear \"demarc\" - Demarcation Point - and will continue to provide good support for issues with the line. This also simplifies setup.</p> <p></p> <p>Above: We do not know whether Vitalik actually approves of this setup</p>"},{"location":"home-router/overview/#which-home-router","title":"Which home router?","text":"<p>Most home routers in the USD 200+ range should do great. Two models are particularly well liked, the Netgear RS line, and the Ubiquity Dream Machine Pro.</p> <p>We've documented Netgear, and are interested in receiving screen shots and a walk-through for Ubiquity. If you're feeling helpful, get in touch on Ethstaker Discord</p> <p>Caution: The Ubiquity Dream Router reportedly does not work well, and cannot handle GBit Internet. If you have one on Comcast in bridge mode, and it works for you, please get in touch so we can document it, including minimum firmware version, any specific settings to make it work, and throughput restrictions.</p>"},{"location":"home-router/overview/#do-all-isps-support-bridge-mode","title":"Do all ISPs support bridge mode?","text":"<p>Not all ISPs do.</p> <p>We've tested successfully on:</p> <ul> <li>Comcast (USA)</li> <li>NOS (Portugal)</li> </ul> <p>These ISPs do not have bridge mode, or \"true bridge mode\":</p> <ul> <li>AT&amp;T Fiber (USA). A community solution is to use an Ubiquity Dream Machine Pro, a custom fiber transceiver, and for WiFi, Ubiquity APs. If you have this working or are interested in getting it to work, please get in touch.</li> </ul>"},{"location":"home-router/prepare/","title":"Prepare","text":"<p>Gather some information about your current network, so you are prepared for the change.</p> <ul> <li>Verify your ISP supports bridge mode, and find out how to enable it</li> <li>Document your current setup<ul> <li>WiFi SSID(s) and their passwords</li> <li>Internal (\"LAN\") network address, e.g. <code>10.1.0.0/24</code> or <code>192.168.1.0/24</code></li> <li>DHCP settings, specifically any \"DHCP reservation\" settings. Your node likely has one, a gaming console may as well</li> <li>Do you have any devices with a static IP? Your node might use a static IP, instead of a DHCP reservation.</li> <li>Port forwarding settings. Your node likely has some, gaming consoles may as well. This may be in an app, not configurable via your ISP router's web interface. Be sure to write down the port number, as well as the protocol: Is it TCP, UDP, or both TCP and UDP?</li> <li>Is IPv6 configured? Note down anything that doesn't look like \"default\" settings</li> </ul> </li> <li>Verify you have logins to everything you need<ul> <li>Your ISP's website</li> <li>Your ISP router's web interface, if any</li> <li>Your ISP's app, if any</li> <li>Your new router's web interface</li> </ul> </li> <li>Make a plan<ul> <li>Are you going to keep your current LAN network address?<ul> <li>Pro: Any static IPs can remain as-is</li> <li>Con: You'd need to set up your new router isolated, so it doesn't conflict with the existing network during initial setup</li> <li>Ethstaker have documented not keeping the current LAN network address, and relying on DHCP reservation</li> </ul> </li> <li>Read through the documentation and ensure you are comfortable, before you take the final step of activating your new router, and switching your ISP router to \"bridge mode\"</li> </ul> </li> </ul> <p>On the left, you'll find navigation for router setup and final switchover.</p>"},{"location":"home-router/rs300-advanced/","title":"Netgear RS Port Forwarding Setup","text":"<p>With the initial setup, you have functioning WiFi and LAN. Any port forwards aren't set up yet, though, and that impacts both your Ethereum node and any gaming consoles. So let's take those additional steps.</p>"},{"location":"home-router/rs300-advanced/#wifi","title":"WiFi","text":"<p>If you did not configure the original WiFi SSID - network name - and password during initial RS setup, do so now. Netgear have instructions.</p>"},{"location":"home-router/rs300-advanced/#dhcp-reservation","title":"DHCP Reservation","text":"<p>This step assumes that all devices that need an unchanging IP address for port forwarding - your Ethereum node, your gaming console(s), anything else - use a DHCP reservation, not a static IP.</p> <p>If it's a static IP configured on the device, you may need to change it there, if the internal network address changes on Netgear, vs. what your ISP router runs.</p> <p>Connect your Ethereum node to a LAN port - local network port - on the Netgear RS, so it gets an IP address from there and we can set up a reservation. One of the 2.5G ports is a good choice.</p> <p>In the Netgear web interface, go to Advanced -&gt; Setup -&gt; LAN Setup, and under \"Address Reservation\", click the Selection radio button to the left of your node entry, then click \"Add\".</p> <p>If you don't see your node there, it didn't get an IP address from the Netgear RS. Is it maybe using a static IP? In that case, you'd set the static IP to something that's in the <code>192.168.1.0/24</code> range of the Netgear RS, and change the DHCP range of the Netgear RS to exclude that static IP.</p> <p></p> <p>Above: Add the DHCP Reservation</p> <p>Repeat this for any other devices in your network that had a \"DHCP Reservation\".</p> <p>Once added, \"Apply\" the changes and see that they were successful.</p> <p></p> <p>Above: DHCP Reservation successful</p>"},{"location":"home-router/rs300-advanced/#port-forwarding","title":"Port Forwarding","text":"<p>Next, let's configure port forwarding for your node, any gaming consoles, and any other port forwarding settings you recorded. When in doubt, check the current configuration of your ISP router once more.</p> <ul> <li>Go to Advanced -&gt; Advanced Setup -&gt; Port Forwarding / Port Triggering.</li> <li>Leave the service type as \"Port Forwarding\".</li> <li>Click on \"Add Custom Service\"</li> <li>Give the port forwarding a name</li> <li>Set the \"Protocol\", for example \"TCP/UDP\" for Ethereum nodes. Prysm is special however, and you may have different TCP and UDP ports. Watch out for that.</li> <li>Set the \"External port range\" to what you had jotted down. Common defaults are <code>9000</code> for a consensus layer client, or <code>9001</code> if using RocketPool. And <code>30303</code> for an execution layer client.</li> <li>\"Use the same port range for Internal port\" should be checked</li> <li>\"Internal IP address\" is the address you reserved for your node in the previous step</li> <li>Click \"Apply\"</li> </ul> <p></p> <p>*Above: Port Forwarding Settings up to External port range</p> <p></p> <p>*Above: The rest of the Port Forwarding Settings</p> <p></p> <p>Above: Port Forwarding successfully set up, here for a RocketPool node</p> <p>Repeat this for any other port forwardings you may have configured in your network, on your current ISP router.</p>"},{"location":"home-router/rs300-start/","title":"Initial Setup","text":"<p>We've tested and documented switching to a Netgear RS300 on Comcast. Any router in the Netgear RS line should work the same way.</p> <p>The Netgear routers have a mobile app as well as a web interface. We've documented setting it up via the web interface from a laptop.</p> <ul> <li>App instructions</li> <li>Laptop instructions</li> </ul> <p>Where it says \"connect your modem to the yellow Internet port on the back of your NETGEAR router\", the \"modem\" here is your current ISP router. Use one of its Ethernet LAN, local network, ports, to connect to the \"Internet\" port on the Netgear RS.</p> <p>Netgear will prompt you for an admin password, download a firmware update, and restart. At that point, you should be able to log in with the new admin password and continue setup.</p> <p>You may also have the option to set a new WiFi SSID and password. These should be set to the existing SSID and password from your ISP's router, so that you don't have to reconfigure any devices in your network.</p> <p>Netgear defaults to an internal <code>192.168.1.0/24</code> network. We haven't tested what happens if that also happens to be the default network of the ISP router. It's possible firmware update will fail, or that you'll need to set up the Netgear router without Internet connection, and connect it only after the ISP router has been switched to \"bridge mode\". Firmware update in that case would happen as the last step, not the first step.</p>"}]}